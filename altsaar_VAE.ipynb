{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-16-f45a6543e2c9>:147: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/nathan/miniconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/nathan/miniconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /home/nathan/miniconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/dat/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /home/nathan/miniconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/dat/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/nathan/miniconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/dat/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/dat/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/nathan/miniconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Saving TensorBoard summaries and images to: /tmp/log/\n",
      "Iteration: 0 ELBO: -524.658 s/iter: 3.328e-04\n",
      "Iteration: 1000 ELBO: -131.451 s/iter: 1.203e-02\n",
      "Iteration: 2000 ELBO: -107.208 s/iter: 1.181e-02\n",
      "Iteration: 3000 ELBO: -104.099 s/iter: 1.062e-02\n",
      "Iteration: 4000 ELBO: -91.228 s/iter: 1.266e-02\n",
      "Iteration: 5000 ELBO: -93.525 s/iter: 1.262e-02\n",
      "Iteration: 6000 ELBO: -95.295 s/iter: 1.126e-02\n",
      "Iteration: 7000 ELBO: -95.227 s/iter: 1.087e-02\n",
      "Iteration: 8000 ELBO: -90.279 s/iter: 1.082e-02\n",
      "Iteration: 9000 ELBO: -92.430 s/iter: 1.075e-02\n",
      "Iteration: 10000 ELBO: -91.046 s/iter: 1.077e-02\n",
      "Iteration: 11000 ELBO: -89.014 s/iter: 1.081e-02\n",
      "Iteration: 12000 ELBO: -88.290 s/iter: 1.081e-02\n",
      "Iteration: 13000 ELBO: -83.967 s/iter: 1.092e-02\n",
      "Iteration: 14000 ELBO: -88.279 s/iter: 1.086e-02\n",
      "Iteration: 15000 ELBO: -83.576 s/iter: 1.088e-02\n",
      "Iteration: 16000 ELBO: -92.387 s/iter: 1.084e-02\n",
      "Iteration: 17000 ELBO: -88.275 s/iter: 1.085e-02\n",
      "Iteration: 18000 ELBO: -86.569 s/iter: 1.092e-02\n",
      "Iteration: 19000 ELBO: -90.466 s/iter: 1.396e-02\n",
      "Iteration: 20000 ELBO: -87.937 s/iter: 1.492e-02\n",
      "Iteration: 21000 ELBO: -88.342 s/iter: 1.155e-02\n",
      "Iteration: 22000 ELBO: -87.592 s/iter: 1.134e-02\n",
      "Iteration: 23000 ELBO: -87.953 s/iter: 1.102e-02\n",
      "Iteration: 24000 ELBO: -82.964 s/iter: 1.096e-02\n",
      "Iteration: 25000 ELBO: -87.091 s/iter: 1.103e-02\n",
      "Iteration: 26000 ELBO: -83.093 s/iter: 1.098e-02\n",
      "Iteration: 27000 ELBO: -84.930 s/iter: 1.217e-02\n",
      "Iteration: 28000 ELBO: -85.057 s/iter: 1.128e-02\n",
      "Iteration: 29000 ELBO: -82.282 s/iter: 1.121e-02\n",
      "Iteration: 30000 ELBO: -85.472 s/iter: 1.104e-02\n",
      "Iteration: 31000 ELBO: -90.023 s/iter: 1.119e-02\n",
      "Iteration: 32000 ELBO: -91.172 s/iter: 1.103e-02\n",
      "Iteration: 33000 ELBO: -90.769 s/iter: 1.106e-02\n",
      "Iteration: 34000 ELBO: -89.283 s/iter: 1.122e-02\n",
      "Iteration: 35000 ELBO: -87.938 s/iter: 1.103e-02\n",
      "Iteration: 36000 ELBO: -84.066 s/iter: 1.103e-02\n",
      "Iteration: 37000 ELBO: -82.504 s/iter: 1.122e-02\n",
      "Iteration: 38000 ELBO: -81.545 s/iter: 1.106e-02\n",
      "Iteration: 39000 ELBO: -83.037 s/iter: 1.104e-02\n",
      "Iteration: 40000 ELBO: -83.554 s/iter: 1.107e-02\n",
      "Iteration: 41000 ELBO: -81.722 s/iter: 1.100e-02\n",
      "Iteration: 42000 ELBO: -88.785 s/iter: 1.110e-02\n",
      "Iteration: 43000 ELBO: -85.758 s/iter: 1.124e-02\n",
      "Iteration: 44000 ELBO: -90.126 s/iter: 1.096e-02\n",
      "Iteration: 45000 ELBO: -87.608 s/iter: 1.103e-02\n",
      "Iteration: 46000 ELBO: -88.598 s/iter: 1.213e-02\n",
      "Iteration: 47000 ELBO: -85.563 s/iter: 1.149e-02\n",
      "Iteration: 48000 ELBO: -86.840 s/iter: 1.130e-02\n",
      "Iteration: 49000 ELBO: -78.272 s/iter: 1.560e-02\n",
      "Iteration: 50000 ELBO: -81.175 s/iter: 1.200e-02\n",
      "Iteration: 51000 ELBO: -85.429 s/iter: 1.170e-02\n",
      "Iteration: 52000 ELBO: -88.293 s/iter: 1.218e-02\n",
      "Iteration: 53000 ELBO: -92.279 s/iter: 1.244e-02\n",
      "Iteration: 54000 ELBO: -80.988 s/iter: 1.258e-02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f45a6543e2c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-f45a6543e2c9>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDeleteRecursively\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMakeDirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-f45a6543e2c9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0mnp_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0mnp_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp_x\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp_x\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;31m# Print progress and save samples every so often\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "#from scipy.misc import imsave\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "distributions = tf.distributions\n",
    "\n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_string('data_dir', '/tmp/dat/', 'Directory for data')\n",
    "flags.DEFINE_string('logdir', '/tmp/log/', 'Directory for logs')\n",
    "\n",
    "# For making plots:\n",
    "# flags.DEFINE_integer('latent_dim', 2, 'Latent dimensionality of model')\n",
    "# flags.DEFINE_integer('batch_size', 64, 'Minibatch size')\n",
    "# flags.DEFINE_integer('n_samples', 10, 'Number of samples to save')\n",
    "# flags.DEFINE_integer('print_every', 10, 'Print every n iterations')\n",
    "# flags.DEFINE_integer('hidden_size', 200, 'Hidden size for neural networks')\n",
    "# flags.DEFINE_integer('n_iterations', 1000, 'number of iterations')\n",
    "\n",
    "# For bigger model:\n",
    "flags.DEFINE_integer('latent_dim', 100, 'Latent dimensionality of model')\n",
    "flags.DEFINE_integer('batch_size', 64, 'Minibatch size')\n",
    "flags.DEFINE_integer('n_samples', 1, 'Number of samples to save')\n",
    "flags.DEFINE_integer('print_every', 1000, 'Print every n iterations')\n",
    "flags.DEFINE_integer('hidden_size', 200, 'Hidden size for neural networks')\n",
    "flags.DEFINE_integer('n_iterations', 100000, 'number of iterations')\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "def inference_network(x, latent_dim, hidden_size):\n",
    "  \"\"\"Construct an inference network parametrizing a Gaussian.\n",
    "  Args:\n",
    "    x: A batch of MNIST digits.\n",
    "    latent_dim: The latent dimensionality.\n",
    "    hidden_size: The size of the neural net hidden layers.\n",
    "  Returns:\n",
    "    mu: Mean parameters for the variational family Normal\n",
    "    sigma: Standard deviation parameters for the variational family Normal\n",
    "  \"\"\"\n",
    "  with slim.arg_scope([slim.fully_connected], activation_fn=tf.nn.relu):\n",
    "    net = slim.flatten(x)\n",
    "    net = slim.fully_connected(net, hidden_size)\n",
    "    net = slim.fully_connected(net, hidden_size)\n",
    "    gaussian_params = slim.fully_connected(\n",
    "        net, latent_dim * 2, activation_fn=None)\n",
    "  # The mean parameter is unconstrained\n",
    "  mu = gaussian_params[:, :latent_dim]\n",
    "  # The standard deviation must be positive. Parametrize with a softplus\n",
    "  sigma = tf.nn.softplus(gaussian_params[:, latent_dim:])\n",
    "  return mu, sigma\n",
    "\n",
    "\n",
    "def generative_network(z, hidden_size):\n",
    "  \"\"\"Build a generative network parametrizing the likelihood of the data\n",
    "  Args:\n",
    "    z: Samples of latent variables\n",
    "    hidden_size: Size of the hidden state of the neural net\n",
    "  Returns:\n",
    "    bernoulli_logits: logits for the Bernoulli likelihood of the data\n",
    "  \"\"\"\n",
    "  with slim.arg_scope([slim.fully_connected], activation_fn=tf.nn.relu):\n",
    "    net = slim.fully_connected(z, hidden_size)\n",
    "    net = slim.fully_connected(net, hidden_size)\n",
    "    bernoulli_logits = slim.fully_connected(net, 784, activation_fn=None)\n",
    "    bernoulli_logits = tf.reshape(bernoulli_logits, [-1, 28, 28, 1])\n",
    "  return bernoulli_logits\n",
    "\n",
    "\n",
    "def train():\n",
    "  # Train a Variational Autoencoder on MNIST\n",
    "\n",
    "  # Input placeholders\n",
    "  with tf.name_scope('data'):\n",
    "    x = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    tf.summary.image('data', x)\n",
    "\n",
    "  with tf.variable_scope('variational'):\n",
    "    q_mu, q_sigma = inference_network(x=x,\n",
    "                                      latent_dim=FLAGS.latent_dim,\n",
    "                                      hidden_size=FLAGS.hidden_size)\n",
    "    # The variational distribution is a Normal with mean and standard\n",
    "    # deviation given by the inference network\n",
    "    q_z = distributions.Normal(loc=q_mu, scale=q_sigma)\n",
    "    assert q_z.reparameterization_type == distributions.FULLY_REPARAMETERIZED\n",
    "\n",
    "  with tf.variable_scope('model'):\n",
    "    # The likelihood is Bernoulli-distributed with logits given by the\n",
    "    # generative network\n",
    "    p_x_given_z_logits = generative_network(z=q_z.sample(),\n",
    "                                            hidden_size=FLAGS.hidden_size)\n",
    "    p_x_given_z = distributions.Bernoulli(logits=p_x_given_z_logits)\n",
    "    posterior_predictive_samples = p_x_given_z.sample()\n",
    "    tf.summary.image('posterior_predictive',\n",
    "                     tf.cast(posterior_predictive_samples, tf.float32))\n",
    "\n",
    "  # Take samples from the prior\n",
    "  with tf.variable_scope('model', reuse=True):\n",
    "    p_z = distributions.Normal(loc=np.zeros(FLAGS.latent_dim, dtype=np.float32),\n",
    "                               scale=np.ones(FLAGS.latent_dim, dtype=np.float32))\n",
    "    p_z_sample = p_z.sample(FLAGS.n_samples)\n",
    "    p_x_given_z_logits = generative_network(z=p_z_sample,\n",
    "                                            hidden_size=FLAGS.hidden_size)\n",
    "    prior_predictive = distributions.Bernoulli(logits=p_x_given_z_logits)\n",
    "    prior_predictive_samples = prior_predictive.sample()\n",
    "    tf.summary.image('prior_predictive',\n",
    "                     tf.cast(prior_predictive_samples, tf.float32))\n",
    "\n",
    "  # Take samples from the prior with a placeholder\n",
    "  with tf.variable_scope('model', reuse=True):\n",
    "    z_input = tf.placeholder(tf.float32, [None, FLAGS.latent_dim])\n",
    "    p_x_given_z_logits = generative_network(z=z_input,\n",
    "                                            hidden_size=FLAGS.hidden_size)\n",
    "    prior_predictive_inp = distributions.Bernoulli(logits=p_x_given_z_logits)\n",
    "    prior_predictive_inp_sample = prior_predictive_inp.sample()\n",
    "\n",
    "  # Build the evidence lower bound (ELBO) or the negative loss\n",
    "  kl = tf.reduce_sum(distributions.kl_divergence(q_z, p_z), 1)\n",
    "  expected_log_likelihood = tf.reduce_sum(p_x_given_z.log_prob(x),\n",
    "                                          [1, 2, 3])\n",
    "\n",
    "  elbo = tf.reduce_sum(expected_log_likelihood - kl, 0)\n",
    "\n",
    "  optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001)\n",
    "\n",
    "  train_op = optimizer.minimize(-elbo)\n",
    "\n",
    "  # Merge all the summaries\n",
    "  summary_op = tf.summary.merge_all()\n",
    "\n",
    "  init_op = tf.global_variables_initializer()\n",
    "\n",
    "  # Run training\n",
    "  sess = tf.InteractiveSession()\n",
    "  sess.run(init_op)\n",
    "\n",
    "  mnist = read_data_sets(FLAGS.data_dir, one_hot=True)\n",
    "\n",
    "  print('Saving TensorBoard summaries and images to: %s' % FLAGS.logdir)\n",
    "  train_writer = tf.summary.FileWriter(FLAGS.logdir, sess.graph)\n",
    "\n",
    "  # Get fixed MNIST digits for plotting posterior means during training\n",
    "  np_x_fixed, np_y = mnist.test.next_batch(5000)\n",
    "  np_x_fixed = np_x_fixed.reshape(5000, 28, 28, 1)\n",
    "  np_x_fixed = (np_x_fixed > 0.5).astype(np.float32)\n",
    "\n",
    "  t0 = time.time()\n",
    "  for i in range(FLAGS.n_iterations):\n",
    "    # Re-binarize the data at every batch; this improves results\n",
    "    np_x, _ = mnist.train.next_batch(FLAGS.batch_size)\n",
    "    np_x = np_x.reshape(FLAGS.batch_size, 28, 28, 1)\n",
    "    np_x = (np_x > 0.5).astype(np.float32)\n",
    "    sess.run(train_op, {x: np_x})\n",
    "\n",
    "    # Print progress and save samples every so often\n",
    "    if i % FLAGS.print_every == 0:\n",
    "      np_elbo, summary_str = sess.run([elbo, summary_op], {x: np_x})\n",
    "      train_writer.add_summary(summary_str, i)\n",
    "      print('Iteration: {0:d} ELBO: {1:.3f} s/iter: {2:.3e}'.format(\n",
    "          i,\n",
    "          np_elbo / FLAGS.batch_size,\n",
    "          (time.time() - t0) / FLAGS.print_every))\n",
    "      t0 = time.time()\n",
    "\n",
    "      # Save samples\n",
    "      np_posterior_samples, np_prior_samples = sess.run(\n",
    "          [posterior_predictive_samples, prior_predictive_samples], {x: np_x})\n",
    "      for k in range(FLAGS.n_samples):\n",
    "        f_name = os.path.join(\n",
    "            FLAGS.logdir, 'iter_%d_posterior_predictive_%d_data.jpg' % (i, k))\n",
    "        #imsave(f_name, np_x[k, :, :, 0])\n",
    "        f_name = os.path.join(\n",
    "            FLAGS.logdir, 'iter_%d_posterior_predictive_%d_sample.jpg' % (i, k))\n",
    "        #imsave(f_name, np_posterior_samples[k, :, :, 0])\n",
    "        f_name = os.path.join(\n",
    "            FLAGS.logdir, 'iter_%d_prior_predictive_%d.jpg' % (i, k))\n",
    "        #imsave(f_name, np_prior_samples[k, :, :, 0])\n",
    "\n",
    "      # Plot the posterior predictive space\n",
    "      if FLAGS.latent_dim == 2:\n",
    "        np_q_mu = sess.run(q_mu, {x: np_x_fixed})\n",
    "        cmap = mpl.colors.ListedColormap(sns.color_palette(\"husl\"))\n",
    "        f, ax = plt.subplots(1, figsize=(6 * 1.1618, 6))\n",
    "        im = ax.scatter(np_q_mu[:, 0], np_q_mu[:, 1], c=np.argmax(np_y, 1), cmap=cmap,\n",
    "                        alpha=0.7)\n",
    "        ax.set_xlabel('First dimension of sampled latent variable $z_1$')\n",
    "        ax.set_ylabel('Second dimension of sampled latent variable mean $z_2$')\n",
    "        ax.set_xlim([-10., 10.])\n",
    "        ax.set_ylim([-10., 10.])\n",
    "        f.colorbar(im, ax=ax, label='Digit class')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(FLAGS.logdir,\n",
    "                                 'posterior_predictive_map_frame_%d.png' % i))\n",
    "        plt.close()\n",
    "\n",
    "        nx = ny = 20\n",
    "        x_values = np.linspace(-3, 3, nx)\n",
    "        y_values = np.linspace(-3, 3, ny)\n",
    "        canvas = np.empty((28 * ny, 28 * nx))\n",
    "        for ii, yi in enumerate(x_values):\n",
    "          for j, xi in enumerate(y_values):\n",
    "            np_z = np.array([[xi, yi]])\n",
    "            x_mean = sess.run(prior_predictive_inp_sample, {z_input: np_z})\n",
    "            canvas[(nx - ii - 1) * 28:(nx - ii) * 28, j *\n",
    "                   28:(j + 1) * 28] = x_mean[0].reshape(28, 28)\n",
    "        imsave(os.path.join(FLAGS.logdir,\n",
    "                            'prior_predictive_map_frame_%d.png' % i), canvas)\n",
    "        # plt.figure(figsize=(8, 10))\n",
    "        # Xi, Yi = np.meshgrid(x_values, y_values)\n",
    "        # plt.imshow(canvas, origin=\"upper\")\n",
    "        # plt.tight_layout()\n",
    "        # plt.savefig()\n",
    "\n",
    "  # Make the gifs\n",
    "  if FLAGS.latent_dim == 2:\n",
    "    os.system(\n",
    "        'convert -delay 15 -loop 0 {0}/posterior_predictive_map_frame*png {0}/posterior_predictive.gif'\n",
    "        .format(FLAGS.logdir))\n",
    "    os.system(\n",
    "        'convert -delay 15 -loop 0 {0}/prior_predictive_map_frame*png {0}/prior_predictive.gif'\n",
    "        .format(FLAGS.logdir))\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  if tf.gfile.Exists(FLAGS.logdir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.logdir)\n",
    "  tf.gfile.MakeDirs(FLAGS.logdir)\n",
    "  train()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
