{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# based on https://danijar.com/variable-sequence-lengths-in-tensorflow/\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import functools\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import re #regexp\n",
    "\n",
    "from tensorflow.python.ops import rnn_cell, rnn\n",
    "from sklearn.metrics import confusion_matrix #compute confusion_matrix\n",
    "from matplotlib import pyplot as plt #display confusion_matrix\n",
    "\n",
    "family = 'Object' \n",
    "\n",
    "directory = '/home/nathan/Documents/FYP_code/LSTM1_guillermo/'\n",
    "directory_dataset = directory + 'data_nathan/'\n",
    "\n",
    "file_training = \"training.txt\"\n",
    "file_testing = \"testing.txt\" \n",
    "\n",
    "feat_size = 63 #21 joints * 3 dimensions (xyz)\n",
    "\n",
    "batch_size = 20\n",
    "padding_size = 300\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.003\n",
    "n_epochs = 100\n",
    "\n",
    "# Network Parameters\n",
    "seq_max_len = padding_size # Sequence max length\n",
    "n_hidden = 100 # hidden layer num of features\n",
    "max_seq_l = 120\n",
    "\n",
    "p_dropout = 0.2 #1 #0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract action-to-group dictionary & group labels\n",
    "def action_to_group (location, family):\n",
    "\n",
    "    with open(location, 'r') as f:\n",
    "        pattern = '_(\\w+)' #for Object by default\n",
    "        if family == 'Motion': pattern = '(\\w+)_' #add patterns like this as a list\n",
    "        regexp = re.compile(pattern)\n",
    "        get = re.findall( regexp, f.read() )\n",
    "\n",
    "    atog = list() #action to group dictionary \n",
    "    g_labels = list() \n",
    "    for i in get:\n",
    "        #check if group already exists\n",
    "        if i not in g_labels: #extract group number as index of word in submitted & add it to action group dictionary\n",
    "            g_labels.append(i)\n",
    "        atog.append( g_labels.index(i) ) \n",
    "\n",
    "    #num_g = len(g_labels) FYI\n",
    "    \n",
    "    return (atog, g_labels) \n",
    "\n",
    "def read_data(filename):\n",
    "    # Reads file containing features and returns features indexed by time\n",
    "    x = []\n",
    "    tmp_length = 0\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            numbers_str = line.split()\n",
    "            nums_float = [float(a) for a in numbers_str]\n",
    "            x.append(nums_float)\n",
    "            tmp_length =tmp_length+1\n",
    "            # print(len(x))\n",
    "    # x.extend([[0.0]*feat_size]*(padding_size-len(x)+1))\n",
    "    f.close() #necessary ? supposed to be automatic\n",
    "    tmp_val = np.min([tmp_length-1,max_seq_l])\n",
    "    #print(tmp_val)\n",
    "    return x[1:]  # ignore de first line (num of frames)\n",
    "\n",
    "def read_config(filename):\n",
    "    # Reads config file and returns filenames and class label\n",
    "    x = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            line_split = line.split()\n",
    "            x.append(line_split)\n",
    "    f.close()\n",
    "    return x\n",
    "\n",
    "# one hot encoding\n",
    "def num_to_idx(num, num_classes):\n",
    "    vec = np.zeros( shape=num_classes, dtype=np.float) #hardcode here\n",
    "    vec[num] = 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Called once to load training/testing data in MODEL\n",
    "def create_dataset(filename, atog, group_number):\n",
    "    # dataset is organized as NxLxD (N = num samples, L temporal length with padding, D feature dimension\n",
    "    # labels is NxY where Y is one hot label vector\n",
    "\n",
    "    dataset, labels, lengths = [], [], []\n",
    "    files = read_config(filename)\n",
    "    \n",
    "    if group_number == -2: #groupRNN to subnets\n",
    "        pass\n",
    "    elif group_number == -1: #main RNN\n",
    "        num_classes = max(atog) + 1 # as labelled 0 to 25 but want 26 as number of classes\n",
    "    else: #subnets\n",
    "        gtoa = group_to_action(atog) \n",
    "        list_actions = gtoa[group_number] #gives list of actions in current group number\n",
    "        num_classes = len(list_actions)\n",
    "        \n",
    "    for i in files:\n",
    "        \n",
    "        if group_number == -2: #Feed output of groupRNN to pre-trained subnet\n",
    "            \n",
    "            tmp_data = read_data(directory_dataset + i[0]) #i[0] =  a01s01r01.txt (for eg)\n",
    "            tmp_labels = int(i[1]) #will one hot out of here as first need to dispatch data to relevant subnet using labels\n",
    "        \n",
    "        elif group_number == -1: #Build group main RNN\n",
    "            \n",
    "            tmp_data = read_data(directory_dataset + i[0]) \n",
    "            num =  atog[ int( i[1] )] #only used for one hot encoding in the line below\n",
    "            tmp_labels = np.transpose(num_to_idx(num, num_classes)) \n",
    "            #i[0] =  a01s01r01.txt (for eg)\n",
    "            #i[1] = number between 0 and 44 (or as many classes there is)\n",
    "            #tmp_labels = [0, 0, 1, 0, .. 0] = one-hot encoding of class value\n",
    "            #tmp_data = list with variable len around 300 & each item in the list is a nested list of len 63 (=feat_size)\n",
    "        \n",
    "        else: #Build action subnet RNN \n",
    "            \n",
    "            num = int( i[1] ) #original action number\n",
    "            if num in list_actions: #select data only if belongs to group \n",
    "                tmp_data = read_data(directory_dataset + i[0]) #add to dataset if part of the group\n",
    "                tmp_labels = np.transpose(num_to_idx(list_actions.index(num), num_classes)) #add labels as well     \n",
    "            else: continue\n",
    "            \n",
    "        if len(tmp_data)<300: #why 300, is that the longest sequence ? 300 = padding_size btw...\n",
    "            \n",
    "            #records tmp_data initial length before padding\n",
    "            #pads tmp_data with zeros until padding_size (300) so len(tmp_data) = 300 always with len 63 items\n",
    "                \n",
    "            lengths.append(len(tmp_data))\n",
    "            tmp_data.extend([ [0.0] * feat_size ] * (padding_size - len(tmp_data)))  \n",
    "\n",
    "            dataset.append(tmp_data)\n",
    "            labels.append(tmp_labels) \n",
    "        \n",
    "    # all 0..565 lists with item as nested lists of size (300, 26, 1)\n",
    "    return dataset, labels, lengths\n",
    "\n",
    "#Called once when training RNN\n",
    "def batch_generation(data,labels,lengths):\n",
    "    \n",
    "    num_classes = np.size(labels,1) #labels is a one hot encoded numpy array. This returns number of columns (=groups).\n",
    "    \n",
    "    nsamples,_,_ = data.shape\n",
    "\n",
    "    indices = np.arange(nsamples)\n",
    "    np.random.shuffle(indices)\n",
    "    num_batches = int(np.floor(nsamples/batch_size))\n",
    "    not_exact = 0\n",
    "\n",
    "    if nsamples%batch_size != 0:\n",
    "        not_exact = 1\n",
    "\n",
    "    batches_data = np.empty(shape=[num_batches+not_exact,batch_size,padding_size,feat_size])\n",
    "    batches_labels = np.empty(shape=[num_batches+not_exact,batch_size,num_classes]) #hardcode here with num_classes\n",
    "    batches_lengths = np.empty(shape=[num_batches + not_exact, batch_size],dtype=np.int32)\n",
    "\n",
    "    for x in range(num_batches):\n",
    "        batches_data[x, :, :, :] = data[indices[batch_size*x:batch_size*(x+1)], :, :]\n",
    "        batches_labels[x,:,:] = labels[indices[batch_size*x:batch_size*(x+1)], :]\n",
    "        batches_lengths[x,:] = lengths[indices[batch_size*x:batch_size*(x+1)]]\n",
    "\n",
    "    if not_exact > 0:\n",
    "        to_complete = nsamples%batch_size\n",
    "        tmp_data = data[indices[batch_size*num_batches:batch_size*num_batches+to_complete],:,:]\n",
    "        tmp_random = np.random.randint(0,nsamples-to_complete,batch_size-to_complete) # we complete last batch with random samples\n",
    "\n",
    "        batches_data[num_batches]=np.concatenate((tmp_data,data[tmp_random,:,:]),axis=0)\n",
    "\n",
    "        tmp_labels = labels[indices[batch_size*num_batches:batch_size*num_batches+to_complete],:]\n",
    "        batches_labels[num_batches] = np.concatenate((tmp_labels,labels[indices[tmp_random],:]))\n",
    "        tmp_lengths = lengths[indices[batch_size*num_batches:batch_size*num_batches+to_complete]]\n",
    "        batches_lengths[num_batches] = np.concatenate((tmp_lengths,lengths[indices[tmp_random]]))\n",
    "\n",
    "    return batches_data, batches_labels, batches_lengths, num_batches+not_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamicRNN(x, seqlen, weights, biases, keep_prob):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Permuting batch_size and n_steps\n",
    "    x = tf.transpose(x, [1, 0, 2])\n",
    "    # Reshaping to (n_steps*batch_size, n_input)\n",
    "    x = tf.reshape(x, [-1, feat_size])\n",
    "    # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.split(x, seq_max_len, 0) # tf.split(value, num_or_size_splits, axis)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "    \n",
    "    #Operator adding dropout to inputs and outputs of the given cell.\n",
    "    lstm = rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob) \n",
    "    \n",
    "    #Added to have 2 layers LSTM, not used here.\n",
    "    layers = 1\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * layers)\n",
    "\n",
    "    # Get lstm cell output, providing 'sequence_length' will perform dynamic calculation.\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cell, x, dtype=tf.float32, sequence_length=seqlen)\n",
    "    \n",
    "    # When performing dynamic calculation, we must retrieve the last\n",
    "    # dynamically computed output, i.e, if a sequence length is 10, we need\n",
    "    # to retrieve the 10th output.\n",
    "    # However TensorFlow doesn't support advanced indexing yet, so we build\n",
    "    # a custom op that for each sample in batch size, get its length and\n",
    "    # get the corresponding relevant output.\n",
    "\n",
    "    # 'outputs' is a list of output at every timestep, we pack them in a Tensor\n",
    "    # and change back dimension to [batch_size, n_step, n_input]\n",
    "    outputs = tf.stack(outputs)\n",
    "    outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "\n",
    "    # Hack to build the indexing and retrieve the right output.\n",
    "    batch_size = tf.shape(outputs)[0]\n",
    "    # Start indices for each sample\n",
    "    index = tf.range(0, batch_size) * seq_max_len + (seqlen - 1)\n",
    "    # Indexing\n",
    "    outputs = tf.gather(tf.reshape(outputs, [-1, n_hidden]), index)\n",
    "\n",
    "    # Linear activation, using outputs computed above\n",
    "    return tf.matmul(outputs, weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doMyRNN(location, family, group_number):\n",
    "\n",
    "    # ==========\n",
    "    #   MODEL\n",
    "    # ==========\n",
    "    print('in MODEL')\n",
    "\n",
    "    tf.reset_default_graph() #Clear computational graph to prevent error\n",
    "\n",
    "    # Load training and testing data\n",
    "    (atog, _) = action_to_group(location, family)\n",
    "    train_data, train_labels, train_lengths = create_dataset(file_training, atog, group_number)\n",
    "    test_data, test_labels, test_lengths = create_dataset(file_testing, atog, group_number)\n",
    "\n",
    "    #cast to numpy array\n",
    "    train_data = np.asarray(train_data)\n",
    "    train_labels = np.asarray(train_labels)\n",
    "    train_lengths = np.asarray(train_lengths,dtype=np.int32)\n",
    "    test_data = np.asarray(test_data)\n",
    "    test_labels = np.asarray(test_labels)\n",
    "    test_lengths = np.asarray(test_lengths)\n",
    "\n",
    "    (samples, rows, row_size) = train_data.shape\n",
    "\n",
    "    n_classes = np.size(train_labels,1)\n",
    "\n",
    "    # tf Graph input\n",
    "    x = tf.placeholder(\"float\", [None, seq_max_len, feat_size], name=\"x\")\n",
    "    y = tf.placeholder(\"float\", [None, n_classes], name=\"y\")\n",
    "\n",
    "    # A placeholder for indicating each sequence length\n",
    "    seqlen = tf.placeholder(tf.int32, [None], name=\"seqlen\")\n",
    "\n",
    "    # Define weights\n",
    "    weights = {\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "    # ==========\n",
    "    #   LAUNCH\n",
    "    # ==========\n",
    "    print('in LAUNCH')\n",
    "\n",
    "    pred = dynamicRNN(x, seqlen, weights, biases, keep_prob)\n",
    "\n",
    "    # Evaluate model \n",
    "    prediction = tf.argmax(pred,1, name=\"prediction\") # for each prediction, keep class with highest level of confidence (tests X classes)\n",
    "    correct_pred = tf.equal(prediction, tf.argmax(y,1)) #output 0 & 1 vector, y is supposed to have true labels\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32)) #mean of above\n",
    "\n",
    "    #Create a saver object which will save all the variables\n",
    "    #saver = tf.train.Saver()\n",
    "\n",
    "    # ============\n",
    "    #   OPTIMIZE\n",
    "    # ============\n",
    "    print('in OPTIMIZE')\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = pred, labels = y))\n",
    "    # optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    # optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Launch the graph\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        sess.run(init)\n",
    "\n",
    "        max_acc = 0.0;\n",
    "        max_epoch = 0;\n",
    "        best_labels = []\n",
    "\n",
    "        # Keep training until reach max iterations\n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "            (batch_x, batch_y, batch_seqlen, n_batches) = batch_generation(train_data, train_labels, train_lengths)\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            for i in range(n_batches):\n",
    "\n",
    "                # Run optimization op (backprop)\n",
    "                sess.run(optimizer, feed_dict={x: batch_x[i,:,:,:], y: batch_y[i,:,:],\n",
    "                                               seqlen: batch_seqlen[i,:], keep_prob: p_dropout})\n",
    "\n",
    "            # Test accuracy on this epoch    \n",
    "            test_acc = sess.run(accuracy, feed_dict={x: test_data, y: test_labels, seqlen: test_lengths, keep_prob: 1.0})\n",
    "\n",
    "            #Save best testing results\n",
    "            if test_acc > max_acc: \n",
    "                \n",
    "                #Save the best accuracy and predictions\n",
    "                max_acc = test_acc; max_epoch = epoch\n",
    "                #gives the confidence score of every class at output along columns (dim 1)\n",
    "                labels = pred.eval(feed_dict={x: test_data, y: test_labels, seqlen: test_lengths, keep_prob: 1.0})\n",
    "                # pred_labels = tf.argmax(labels, 1) OR\n",
    "                pred_labels = prediction.eval(feed_dict={x: test_data, y: test_labels, seqlen: test_lengths, keep_prob: 1.0})\n",
    "                #test_labels is one_hot. Want 1-D like best_labels\n",
    "                true_labels = np.asarray( [ np.where(r==1)[0][0] for r in test_labels ], dtype=np.int64)\n",
    "\n",
    "                #Save best  \n",
    "                saver = tf.train.Saver() #Create a saver object which will save all the variables\n",
    "                location = directory + 'saved_sessions/subnets/' + family + '/subnet_' + str(group_number)\n",
    "                saver.save(sess, location)\n",
    "                print('Saved subnet at epoch ' + str(epoch) + ' at ' + location)\n",
    "\n",
    "            print('Epoch {:2d} accuracy {:3.1f}% in {:3.1f} seconds'.format(epoch, 100 * test_acc, time.time() - start))\n",
    "            print('max_acc {:3.1f}% at epoch {:2d} \\n'.format(max_acc*100, max_epoch))\n",
    "            \n",
    "            #Stop training when accuracy is maximum\n",
    "            if max_acc == 1: \n",
    "                print('Reached 100% accuracy -> exit training \\n')\n",
    "                break\n",
    "             \n",
    "        print (\"Optimization Finished!\")\n",
    "\n",
    "        return (labels, pred_labels, true_labels)\n",
    "\n",
    "        # ToDo\n",
    "        # Get training accuracy and plot it along test accuracy \n",
    "        # Add a validation set accuracy to train without overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display confusion matrix\n",
    "def confusion_mtx(true, pred, name_labels):\n",
    "    \n",
    "    num_classes = len(name_labels)\n",
    "    cm = confusion_matrix(true, pred)\n",
    "\n",
    "    fig = plt.figure(figsize=(10,10), dpi=100)\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(cm)\n",
    "    fig.colorbar(cax)\n",
    "    plt.xticks(range(num_classes), name_labels, rotation=90)\n",
    "    plt.yticks(range(num_classes), name_labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "    \n",
    "    #TODO\n",
    "    #Adjust the matrix scale (0 to 45 to the relevant scaling 0 to 100%)\n",
    "    #Have a checkerboard figure to track classes easily\n",
    "    #Have white background for better error reading for colorbar\n",
    "    #Rotate 45 the vertical axis\n",
    "    #Write number of successful matches inside the box (especially good for subnets)\n",
    "\n",
    "#Display confidence level bar histogram \n",
    "def confidence_probability_g (scores, pred_class, true_class, g_labels):\n",
    "    \n",
    "    format_scores = scores - np.amin(scores)\n",
    "    format_scores = format_scores / np.amax(format_scores)\n",
    "    \n",
    "    #y = scores\n",
    "    y = format_scores #probabilities made from normalising on this 1 score (not on the 569 scores)\n",
    "    x = range(len(scores)) #as much scores as there is in y (45)\n",
    "    \n",
    "    plt.figure(figsize=(20,4))\n",
    "    \n",
    "    barlist = plt.bar(x,y, width=0.7)\n",
    "\n",
    "    import randomcolor\n",
    "    colors = randomcolor.RandomColor().generate(count=len(g_labels)) #as many group or actions    \n",
    "    for i in range(len(barlist)): barlist[i].set_color(colors[i])\n",
    "    \n",
    "    plt.title('Predicted class ' + str(pred_class) + ' ' + g_labels[pred_class] \n",
    "                 + ' for class ' + str(true_class) + ' ' + g_labels[true_class],\n",
    "                 fontsize=18)\n",
    "    \n",
    "    plt.xticks(x, g_labels, rotation=90, fontsize=18)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    #ToDo\n",
    "    # Be able to know the name of sample seq that fails "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_labels(group_number):\n",
    "    with open(directory + 'labels/name_of_labels_original.txt') as f:\n",
    "        all_45_action_labels = [word for line in f for word in line.split()]\n",
    "    action_labels_inside_this_group = [all_45_action_labels[i] for i in range(len(atog)) if atog[i] == group_number]\n",
    "    return action_labels_inside_this_group\n",
    "\n",
    "def group_to_action(atog):\n",
    "    gtoa = []\n",
    "    for group_number in range(max(atog)+1):\n",
    "        gtoa.append( [index for index, value in enumerate(atog) if value == group_number] )\n",
    "    return gtoa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train group RNN on classifying the object group\n",
    "\n",
    "#Load group dico and labels\n",
    "location = directory + 'labels/grouping_1/object_group_labels.txt'\n",
    "\n",
    "(labels, pred_labels, true_labels) = doMyRNN(location, family, -1) #-1 as this is a main group net (not subnet)\n",
    "\n",
    "#Save on disk to not have to retrain to get results\n",
    "location = directory + 'results/group_RNN/' + family + '/'\n",
    "np.save(location + 'labels.npy', labels)    # .npy extension is added if not given\n",
    "np.save(location + 'pred_labels.npy', pred_labels) \n",
    "np.save(location + 'true_labels.npy', true_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyze Object group RNN results\n",
    "\n",
    "#Load variables to analyze\n",
    "location = directory + 'results/group_RNN/' + family + '/'\n",
    "(labels, pred_labels, true_labels) = ( np.load(location + 'labels.npy'), \n",
    "                                       np.load(location + 'pred_labels.npy'),\n",
    "                                       np.load(location + 'true_labels.npy') )\n",
    "location = directory + 'labels/grouping_1/object_group_labels.txt'\n",
    "(_, g_labels) = action_to_group(location, family)\n",
    "\n",
    "# Run analysis: score, accuracy, conf_mat\n",
    "ite = 7\n",
    "confidence_probability_g (labels[ite,:], pred_labels[ite], true_labels[ite], g_labels)\n",
    "correct_pred = np.equal(true_labels, pred_labels) #output 0 & 1 vector\n",
    "acc = np.mean(correct_pred) #mean of above\n",
    "print('Group %s Accuracy %.2f %% with %i groups' % (family, float(100*acc), len(g_labels)))\n",
    "confusion_mtx(true_labels, pred_labels, g_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Subnets are below **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train subnets independently of group RNN \n",
    "\n",
    "#To visualise the whole grouping of 45 actions at once as a nested list\n",
    "\"\"\"\n",
    "for i in range(26):\n",
    "    group_sub_labels = get_group_labels(i)\n",
    "    print(group_sub_labels)\n",
    "\"\"\"\n",
    "\n",
    "#Subnets are just like the orginal RNN but with a selected number of training samples from training.txt and testing.txt\n",
    "\n",
    "location = directory + 'labels/grouping_1/object_group_labels.txt'\n",
    "(atog, g_labels) = action_to_group(location, family)\n",
    "\n",
    "#do a for group_number in range(max(atog)+1):\n",
    "group_number = 3 \n",
    "num_classes = atog.count(group_number) #number of classes\n",
    "group_sub_labels = get_group_labels(group_number) #labels of classes (inside this group)\n",
    "\n",
    "if num_classes > 1: #don't train RNN with 1 output\n",
    "    \n",
    "    (labels, pred_labels, true_labels) = doMyRNN(location, family, group_number)\n",
    "\n",
    "    #Evaluating subnet\n",
    "    acc = np.mean(np.equal(true_labels, pred_labels))\n",
    "    print('Group %i \"%s\" - %s - accuracy: %.2f %%' % (group_number, g_labels[group_number], group_sub_labels, float(100*acc)))\n",
    "    print(pred_labels)\n",
    "    print(true_labels)\n",
    "    print(np.equal(true_labels, pred_labels))\n",
    "    confusion_mtx(true_labels, pred_labels, group_sub_labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Train all subnets with a for loop and save them all\n",
    "#2. Import each as pre-trained model\n",
    "#3. Gather relevant output of groupRNN \n",
    "#4. Feed it to subnet\n",
    "#5. # Get accuracy of each subnet and overall accuracy, including the outputs that did not get a subnet (1 action groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain all original data\n",
    "(test_data, test_labels, test_lengths) = create_dataset(file_testing, [], -2)\n",
    "test_data, test_labels, test_lengths = np.asarray(test_data), np.asarray(test_labels), np.asarray(test_lengths) \n",
    "\n",
    "#Link groupRNN output to subnets input using pred_labels\n",
    "location = directory + 'results/group_RNN/' + family + '/'\n",
    "pred_labels = np.load(location + 'pred_labels.npy') \n",
    "indices = [index for index, value in enumerate(pred_labels) if value == group_number]\n",
    "\n",
    "#Start for loop here\n",
    "(sub_test_data, sub_test_labels, sub_test_lengths) = test_data[indices], test_labels[indices], test_lengths[indices]\n",
    "#use sub_test_labels to compare at the end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load pre-trained subnet\n",
    "sess=tf.Session()    \n",
    "location = directory + 'saved_sessions/subnets/' + family + '/subnet_' + str(group_number)\n",
    "saver = tf.train.import_meta_graph(location + \".meta\")\n",
    "saver.restore(sess, location)\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "#Import variables\n",
    "x = graph.get_tensor_by_name(\"x:0\")\n",
    "y = graph.get_tensor_by_name(\"y:0\")\n",
    "seqlen = graph.get_tensor_by_name(\"seqlen:0\")\n",
    "keep_prob = graph.get_tensor_by_name(\"keep_prob:0\")\n",
    "#Maybe weights need to be loaded and put into feed_dict.. check ???\n",
    "\n",
    "#dont need to pass labels normally as they are not used\n",
    "#one_hot the label as not done in create_dataset for this part\n",
    "#sub_test_labels = np.asarray( num_to_idx(???num, ???num_classes) )\n",
    "#if not true, add 'y' to feed_dict\n",
    "\n",
    "feed_dict={x: sub_test_data, seqlen: sub_test_lengths, keep_prob: 1.0} \n",
    "prediction = graph.get_tensor_by_name(\"prediction:0\")\n",
    "#sub_pred_labels = prediction.eval(feed_dict)\n",
    "sub_pred_labels = sess.run(prediction, feed_dict) \n",
    "\n",
    "print(sub_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtoa = group_to_action(atog); list_actions = gtoa[group_number] \n",
    "original_sub_pred_labels = [list_actions[i] for i in sub_pred_labels]\n",
    "\n",
    "print(sub_pred_labels)\n",
    "print(original_sub_pred_labels)\n",
    "\n",
    "correct_pred = np.equal(original_sub_pred_labels, sub_pred_labels) #output 0 & 1 vector\n",
    "#append it to the rest of preds first...\n",
    "#acc = np.mean(correct_pred) #mean of above\n",
    "\n",
    "print(acc)\n",
    "\n",
    "#Only care about overall accuracy of the subnet after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
