{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from myfunctions import action_to_group, get_group_labels, read_data, read_config, num_to_idx \n",
    "directory_dataset = '/home/nathan/Documents/FYP_code/LSTM1_guillermo/data_nathan/'\n",
    "\n",
    "feat_size = 63 #21 joints * 3 dimensions (xyz)\n",
    "batch_size = 20\n",
    "padding_size = 300\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.003\n",
    "n_epochs = 200\n",
    "\n",
    "# Network Parameters\n",
    "seq_max_len = padding_size # Sequence max length\n",
    "n_hidden = 100 # hidden layer num of features\n",
    "p_dropout = 0.5\n",
    "\n",
    "#Called to load training/testing data \n",
    "def create_dataset(filename, group_number):\n",
    "    # dataset is organized as NxLxD (N = num samples, L temporal length with padding, D feature dimension\n",
    "    # labels is NxY where Y is one hot label vector\n",
    "    \n",
    "    # -3 sends back labels as actions number (0..45) with one hot enconding (000..1..00)\n",
    "    # -2 sends back labels as actions number (0..45) with no one hot enconding (1 or 2 or 45)\n",
    "    # -1 sends back labels as groups number (0..12or26) with one hot encoding (000..1..00)\n",
    "    # 0> sends back labels of only this group with one hot enconding\n",
    "\n",
    "    dataset, labels, lengths = [], [], []\n",
    "    files = read_config(filename)\n",
    "    \n",
    "    if group_number == -3: #standard 45 actions RNN\n",
    "        num_classes = 45\n",
    "    elif group_number == -2: #groupRNN to subnets (testing phase)\n",
    "        #num_classes = 45 #not needed in the function when -2 activated\n",
    "        pass\n",
    "    elif group_number == -1: #groupRNN\n",
    "        num_classes = max(atog) + 1 \n",
    "        #pass\n",
    "    else: #subnets\n",
    "        list_actions = gtoa[group_number] #gives list of actions in current group number\n",
    "        num_classes = len(list_actions)\n",
    "        \n",
    "    for i in files:\n",
    "        \n",
    "        if group_number == -3:  #standard 45 actions RNN\n",
    "            \n",
    "            tmp_data = read_data(directory_dataset + i[0]) #i[0] =  a01s01r01.txt (for eg)\n",
    "            num =  int( i[1] )\n",
    "            tmp_labels = np.transpose(num_to_idx(num, num_classes)) #one hot encoding\n",
    "        \n",
    "        elif group_number == -2: #Feed output of groupRNN to pre-trained subnet (testing phase)\n",
    "            \n",
    "            tmp_data = read_data(directory_dataset + i[0]) #i[0] =  a01s01r01.txt (for eg)\n",
    "            tmp_labels = int(i[1]) #will one hot later as first need to dispatch data to relevant subnet using labels\n",
    "        \n",
    "        elif group_number == -1: #Build groupRNN\n",
    "            \n",
    "            tmp_data = read_data(directory_dataset + i[0]) \n",
    "            num =  atog[ int( i[1] )] #only used for one hot encoding in the line below\n",
    "            tmp_labels = np.transpose(num_to_idx(num, num_classes)) \n",
    "            #i[0] =  a01s01r01.txt (for eg)\n",
    "            #i[1] = number between 0 and 44 (or as many classes there is)\n",
    "            #tmp_labels = [0, 0, 1, 0, .. 0] = one-hot encoding of class value\n",
    "            #tmp_data = list with variable len around 300 & each item in the list is a nested list of len 63 (=feat_size)\n",
    "        \n",
    "        else: #Build subnet RNN\n",
    "            \n",
    "            num = int( i[1] ) #original action number\n",
    "            if num in list_actions: #select data only if belongs to group \n",
    "                tmp_data = read_data(directory_dataset + i[0]) #add to dataset if part of the group\n",
    "                tmp_labels = np.transpose(num_to_idx(list_actions.index(num), num_classes)) #add labels as well     \n",
    "            else: continue\n",
    "            \n",
    "        if len(tmp_data)<300: #why 300, is that the longest sequence ? 300 = padding_size btw...\n",
    "            \n",
    "            #records tmp_data initial length before padding\n",
    "            #pads tmp_data with zeros until padding_size (300) so len(tmp_data) = 300 always with len 63 items\n",
    "                \n",
    "            lengths.append(len(tmp_data))\n",
    "            tmp_data.extend([ [0.0] * feat_size ] * (padding_size - len(tmp_data)))  \n",
    "\n",
    "            dataset.append(tmp_data)\n",
    "            labels.append(tmp_labels) \n",
    "        \n",
    "    # all 0..565 lists with item as nested lists of size (300, 26, 1)\n",
    "    \n",
    "    return np.asarray(dataset), np.asarray(labels), np.asarray(lengths,dtype=np.int32)\n",
    "\n",
    "#Called once when training RNN\n",
    "def batch_generation(data,labels,lengths):\n",
    "    \n",
    "    num_classes = max(labels)+1 #labels is a one hot encoded numpy array. This returns number of columns (=groups).\n",
    "    \n",
    "    nsamples,_,_ = data.shape\n",
    "\n",
    "    indices = np.arange(nsamples) #np.arange(3) -> array([0, 1, 2])\n",
    "    np.random.shuffle(indices) #shuffle the indices\n",
    "    num_batches = int(np.floor(nsamples/batch_size)) #round to inferior number so = 0 if batch_size bigger than nsamples\n",
    "    not_exact = 0\n",
    "\n",
    "    if nsamples%batch_size != 0: #happens all the time unless nsamples is lucky multiple of batch_size\n",
    "        not_exact = 1\n",
    "    \n",
    "    #declare empty arrays to contain the batches, dimensions are right\n",
    "    batches_data = np.empty(shape=[num_batches+not_exact,batch_size,padding_size,feat_size])\n",
    "    batches_labels = np.empty(shape=[num_batches+not_exact,batch_size,num_classes]) \n",
    "    batches_lengths = np.empty(shape=[num_batches + not_exact, batch_size],dtype=np.int32)\n",
    "\n",
    "    for x in range(num_batches):\n",
    "        batches_data[x, :, :, :] = data[indices[batch_size*x:batch_size*(x+1)], :, :]\n",
    "        batches_labels[x,:,:] = labels[indices[batch_size*x:batch_size*(x+1)], :]\n",
    "        batches_lengths[x,:] = lengths[indices[batch_size*x:batch_size*(x+1)]]\n",
    "\n",
    "    if not_exact > 0:\n",
    "        \n",
    "        to_complete = nsamples%batch_size\n",
    "        \n",
    "        #nsamples is too small, reuse the samples from previous batch, taken randomly to complete this batch\n",
    "        tmp_random = np.random.randint(0,nsamples,batch_size-to_complete) # we complete last batch with random samples\n",
    "        #prints list of indices it will take randomly\n",
    "        \n",
    "        #[num_batches] refers to the last batch that is not complete\n",
    "        tmp_data = data[indices[batch_size*num_batches:batch_size*num_batches+to_complete],:,:]\n",
    "        batches_data[num_batches]=np.concatenate((tmp_data,data[tmp_random,:,:]),axis=0)\n",
    "\n",
    "        tmp_labels = labels[indices[batch_size*num_batches:batch_size*num_batches+to_complete],:]\n",
    "        batches_labels[num_batches] = np.concatenate((tmp_labels,labels[indices[tmp_random],:]))\n",
    "        \n",
    "        tmp_lengths = lengths[indices[batch_size*num_batches:batch_size*num_batches+to_complete]]\n",
    "        batches_lengths[num_batches] = np.concatenate((tmp_lengths,lengths[indices[tmp_random]]))\n",
    "\n",
    "    return batches_data, batches_labels, batches_lengths, num_batches+not_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "('Epoch', 0, 'tes_elbo', -13100.437, 'test_codes[0]', array([-0.71884704,  1.0326077 ], dtype=float32))\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'batch_generation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-82215bfb71f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m##plot_samples(ax[epoch, 1:], test_samples) #not working\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_seqlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_batches\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m#for _ in range(600):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_generation' is not defined"
     ]
    }
   ],
   "source": [
    "# Full example for my blog post at:\n",
    "# https://danijar.com/building-variational-auto-encoders-in-tensorflow/\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tfd = tf.contrib.distributions\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def make_encoder(data, code_size):\n",
    "  x = tf.layers.flatten(data)\n",
    "  x = tf.layers.dense(x, 200, tf.nn.relu)\n",
    "  x = tf.layers.dense(x, 200, tf.nn.relu)\n",
    "  loc = tf.layers.dense(x, code_size)\n",
    "  scale = tf.layers.dense(x, code_size, tf.nn.softplus)\n",
    "  return tfd.MultivariateNormalDiag(loc, scale)\n",
    "\n",
    "\n",
    "def make_prior(code_size):\n",
    "  loc = tf.zeros(code_size)\n",
    "  scale = tf.ones(code_size)\n",
    "  return tfd.MultivariateNormalDiag(loc, scale)\n",
    "\n",
    "\n",
    "def make_decoder(code, data_shape):\n",
    "  x = code\n",
    "  x = tf.layers.dense(x, 200, tf.nn.relu)\n",
    "  x = tf.layers.dense(x, 200, tf.nn.relu)\n",
    "  logit = tf.layers.dense(x, np.prod(data_shape))\n",
    "  logit = tf.reshape(logit, [-1] + data_shape)\n",
    "\n",
    "  #return tfd.Independent(tfd.Bernoulli(logit), 2)\n",
    "  return tfd.Independent( tfd.Normal(loc=0., scale=1.), 63)\n",
    "\n",
    "\n",
    "def plot_codes(ax, codes, labels):\n",
    "  ax.scatter(codes[:, 0], codes[:, 1], s=2, c=labels, alpha=0.1)\n",
    "  ax.set_aspect('equal')\n",
    "  ax.set_xlim(codes.min() - .1, codes.max() + .1)\n",
    "  ax.set_ylim(codes.min() - .1, codes.max() + .1)\n",
    "  ax.tick_params(\n",
    "      axis='both', which='both', left=False, bottom=False,\n",
    "      labelleft=False , labelbottom=False)\n",
    "\n",
    "\n",
    "def plot_samples(ax, samples):\n",
    "  for index, sample in enumerate(samples):\n",
    "    ax[index].imshow(sample, cmap='gray')\n",
    "    ax[index].axis(False)\n",
    "\n",
    "#data = tf.placeholder(tf.float32, [None, 28, 28])\n",
    "data = tf.placeholder(tf.float32, [None, 300, 63])\n",
    "\n",
    "make_encoder = tf.make_template('encoder', make_encoder)\n",
    "make_decoder = tf.make_template('decoder', make_decoder)\n",
    "\n",
    "prior = make_prior(code_size=2)\n",
    "posterior = make_encoder(data, code_size=2)\n",
    "code = posterior.sample()\n",
    "\n",
    "#likelihood = make_decoder(code, [28, 28]).log_prob(data)\n",
    "likelihood = make_decoder(code, [300, 63]).log_prob(data)\n",
    "\n",
    "divergence = tfd.kl_divergence(posterior, prior)\n",
    "elbo = tf.reduce_mean(likelihood - divergence)\n",
    "optimize = tf.train.AdamOptimizer(0.001).minimize(-elbo)\n",
    "\n",
    "#samples = make_decoder(prior.sample(10), [28, 28]).mean()\n",
    "samples = make_decoder(prior.sample(10), [300, 63]).mean()\n",
    "\n",
    "#mnist = input_data.read_data_sets('MNIST_data/')\n",
    "train_data, train_labels, train_lengths = create_dataset(\"training.txt\",-2)\n",
    "(batch_x, batch_y, batch_seqlen, n_batches) = batch_generation(train_data, train_labels, train_lengths)\n",
    "\n",
    "##fig, ax = plt.subplots(nrows=20, ncols=11, figsize=(10, 20))\n",
    "\n",
    "with tf.train.MonitoredSession() as sess:\n",
    "  for epoch in range(20):\n",
    "    \n",
    "    #feed = {data: mnist.test.images.reshape([-1, 28, 28])}\n",
    "    feed = {data: train_data}\n",
    "    \n",
    "    test_elbo, test_codes, test_samples = sess.run([elbo, code, samples], feed)\n",
    "    \n",
    "    print('Epoch', epoch, 'tes_elbo', test_elbo, 'test_codes[0]', test_codes[0])\n",
    "    \n",
    "    ##ax[epoch, 0].set_ylabel('Epoch {}'.format(epoch))\n",
    "    ##plot_codes(ax[epoch, 0], test_codes, mnist.test.labels) #plot codes where ?\n",
    "    ##plot_codes(ax[epoch, 0], test_codes, train_labels)\n",
    "    ##plot_samples(ax[epoch, 1:], test_samples) #not working\n",
    "    \n",
    "    \n",
    "    for i in range(30):\n",
    "    #for _ in range(600):\n",
    "      #feed = {data: mnist.train.next_batch(100)[0].reshape([-1, 28, 28])}\n",
    "      feed = {data: batch_x[i] } #batch_x is (30, 20, 300, 63)\n",
    "    \n",
    "      sess.run(optimize, feed)\n",
    "##plt.savefig('vae-mnist.png', dpi=300, transparent=True, bbox_inches='tight')\n",
    "##plt.savefig('vae-actist.png', dpi=300, transparent=True, bbox_inches='tight') #algo not working so not point saving atm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_codes.shape (10000, 2) #why returns 10000 ?\n",
    "#test_samples.shape (10, 28, 28)\n",
    "#test_elbo.shape ()\n",
    "#mnist.test.images.reshape([-1, 28, 28]) (10000, 28, 28)\n",
    "#mnist.test.labels (10000,)\n",
    "#mnist.train.next_batch(100)[0].shape (100, 784) \n",
    "## 10000 images each of dimensions 28*28, 10 samples of those images per epoch, 2 dimensions for the embedding code.\n",
    "## MNIST:The training set contains 60000 examples, and the test set 10000 examples.\n",
    "# Pass input data as numpy array of dimension (number of samples, dimA, dimB) -> (595, 300, 63)\n",
    "# No labels to do VAE (unsupervised learning)\n",
    "# Ok got it for input.\n",
    "# For distrib ?\n",
    "# For visualisation ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** ORIGINAL VERSION BELOW **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full example for my blog post at:\n",
    "# https://danijar.com/building-variational-auto-encoders-in-tensorflow/\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tfd = tf.contrib.distributions\n",
    "\n",
    "\n",
    "def make_encoder(data, code_size):\n",
    "  x = tf.layers.flatten(data)\n",
    "  x = tf.layers.dense(x, 200, tf.nn.relu)\n",
    "  x = tf.layers.dense(x, 200, tf.nn.relu)\n",
    "  loc = tf.layers.dense(x, code_size)\n",
    "  scale = tf.layers.dense(x, code_size, tf.nn.softplus)\n",
    "  return tfd.MultivariateNormalDiag(loc, scale)\n",
    "\n",
    "\n",
    "def make_prior(code_size):\n",
    "  loc = tf.zeros(code_size)\n",
    "  scale = tf.ones(code_size)\n",
    "  return tfd.MultivariateNormalDiag(loc, scale)\n",
    "\n",
    "\n",
    "def make_decoder(code, data_shape):\n",
    "  x = code\n",
    "  x = tf.layers.dense(x, 200, tf.nn.relu)\n",
    "  x = tf.layers.dense(x, 200, tf.nn.relu)\n",
    "  logit = tf.layers.dense(x, np.prod(data_shape))\n",
    "  logit = tf.reshape(logit, [-1] + data_shape)\n",
    "  return tfd.Independent(tfd.Bernoulli(logit), 2)\n",
    "\n",
    "\n",
    "def plot_codes(ax, codes, labels):\n",
    "  ax.scatter(codes[:, 0], codes[:, 1], s=2, c=labels, alpha=0.1)\n",
    "  ax.set_aspect('equal')\n",
    "  ax.set_xlim(codes.min() - .1, codes.max() + .1)\n",
    "  ax.set_ylim(codes.min() - .1, codes.max() + .1)\n",
    "  ax.tick_params(\n",
    "      axis='both', which='both', left='off', bottom='off',\n",
    "      labelleft='off', labelbottom='off')\n",
    "\n",
    "\n",
    "def plot_samples(ax, samples):\n",
    "  for index, sample in enumerate(samples):\n",
    "    ax[index].imshow(sample, cmap='gray')\n",
    "    ax[index].axis('off')\n",
    "\n",
    "\n",
    "data = tf.placeholder(tf.float32, [None, 28, 28])\n",
    "\n",
    "make_encoder = tf.make_template('encoder', make_encoder)\n",
    "make_decoder = tf.make_template('decoder', make_decoder)\n",
    "\n",
    "# Define the model.\n",
    "prior = make_prior(code_size=2)\n",
    "posterior = make_encoder(data, code_size=2)\n",
    "code = posterior.sample()\n",
    "\n",
    "# Define the loss.\n",
    "likelihood = make_decoder(code, [28, 28]).log_prob(data)\n",
    "divergence = tfd.kl_divergence(posterior, prior)\n",
    "elbo = tf.reduce_mean(likelihood - divergence)\n",
    "optimize = tf.train.AdamOptimizer(0.001).minimize(-elbo)\n",
    "\n",
    "samples = make_decoder(prior.sample(10), [28, 28]).mean()\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data/')\n",
    "fig, ax = plt.subplots(nrows=20, ncols=11, figsize=(10, 20))\n",
    "with tf.train.MonitoredSession() as sess:\n",
    "  for epoch in range(20):\n",
    "    feed = {data: mnist.test.images.reshape([-1, 28, 28])}\n",
    "    test_elbo, test_codes, test_samples = sess.run([elbo, code, samples], feed)\n",
    "    print('Epoch', epoch, 'elbo', test_elbo)\n",
    "    ax[epoch, 0].set_ylabel('Epoch {}'.format(epoch))\n",
    "    plot_codes(ax[epoch, 0], test_codes, mnist.test.labels)\n",
    "    plot_samples(ax[epoch, 1:], test_samples)\n",
    "    for _ in range(600):\n",
    "      feed = {data: mnist.train.next_batch(100)[0].reshape([-1, 28, 28])}\n",
    "      sess.run(optimize, feed)\n",
    "plt.savefig('vae-mnist.png', dpi=300, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 784)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.next_batch(100)[0].shape (100, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/nathan/miniconda3/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets(train=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7f78f5b70c50>, validation=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7f78f5b70a90>, test=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7f78f5b70690>)\n"
     ]
    }
   ],
   "source": [
    "print(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels, train_lengths = create_dataset(\"training.txt\",-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-c9afc8cbbbb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_seqlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_batches\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-3cb92692e777>\u001b[0m in \u001b[0;36mbatch_generation\u001b[0;34m(data, labels, lengths)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mbatches_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mbatches_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0mbatches_lengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "(batch_x, batch_y, batch_seqlen, n_batches) = batch_generation(train_data, train_labels, train_lengths)\n",
    "print(n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data/')\n",
    "\n",
    "mnist.train.next_batch(100)[0].reshape([-1, 28, 28]).shape #(10000, 28, 28)\n",
    "mnist.test.images.reshape([-1, 28, 28])[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(595,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
