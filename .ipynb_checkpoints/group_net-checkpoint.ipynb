{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 classes: [use, clean, close, do, flip, meet, open, pour, prick, sprinkle, insert, takeout, squeeze]\n",
      "\n",
      "['charge_cell_phone', 'light_candle', 'use_calculator', 'use_flash', 'write']\n",
      "['clean_glasses', 'wash_sponge']\n",
      "['close_juice_bottle', 'close_liquid_soap', 'close_milk', 'close_peanut_butter']\n",
      "['drink_mug', 'scratch_sponge', 'stir', 'tear_paper']\n",
      "['flip_pages', 'flip_sponge']\n",
      "['give_card', 'give_coin', 'handshake', 'high_five', 'receive_coin', 'toast_wine']\n",
      "['open_juice_bottle', 'open_letter', 'open_liquid_soap', 'open_milk', 'open_peanut_butter', 'open_soda_can', 'open_wallet']\n",
      "['pour_juice_bottle', 'pour_liquid_soap', 'pour_milk', 'pour_wine']\n",
      "['prick', 'scoop_spoon']\n",
      "['put_salt', 'sprinkle']\n",
      "['put_sugar', 'put_tea_bag']\n",
      "['read_letter', 'take_letter_from_enveloppe', 'unfold_glasses']\n",
      "['squeeze_paper', 'squeeze_sponge']\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# based on https://danijar.com/variable-sequence-lengths-in-tensorflow/\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import functools\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os #os.environ\n",
    "\n",
    "from tensorflow.python.ops import rnn_cell, rnn\n",
    "from sklearn.metrics import confusion_matrix #compute confusion_matrix\n",
    "from matplotlib import pyplot as plt #display confusion_matrix\n",
    "from sklearn.model_selection import train_test_split #for training/validation set creation\n",
    "from myfunctions import action_to_group, get_group_labels, read_data, read_config, num_to_idx \n",
    "\n",
    "#family = 'Object' \n",
    "family ='Motion'\n",
    "\n",
    "#directory = '/home/nathan/Documents/FYP_code/LSTM1_guillermo/grouping_1/'\n",
    "directory = '/home/nathan/Documents/FYP_code/LSTM1_guillermo/grouping_2/'\n",
    "\n",
    "label_location = directory + 'labels/' + family + '_group_labels.txt' \n",
    "directory_dataset = '/home/nathan/Documents/FYP_code/LSTM1_guillermo/data_nathan/'\n",
    "file_training = \"training.txt\"\n",
    "file_testing = \"testing.txt\" \n",
    "\n",
    "feat_size = 63 #21 joints * 3 dimensions (xyz)\n",
    "batch_size = 20\n",
    "padding_size = 300\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.003\n",
    "n_epochs = 200\n",
    "\n",
    "# Network Parameters\n",
    "seq_max_len = padding_size # Sequence max length\n",
    "n_hidden = 100 # hidden layer num of features\n",
    "p_dropout = 0.5\n",
    "\n",
    "#Load global variables (used everywhere)\n",
    "(atog, gtoa, g_labels) = action_to_group(label_location, family)\n",
    "groups_with_subnet =  [i for i, value in enumerate(gtoa) if len(value)>1]\n",
    "groups_without_subnet =  [i for i, value in enumerate(gtoa) if len(value)==1]\n",
    "num_classes = max(atog) + 1 #or len(g_labels) \n",
    "\n",
    "#Sanity check\n",
    "print (str(num_classes)+ ' classes: [' + ', '.join(g_labels) + ']\\n')\n",
    "location = directory + 'labels/name_of_labels_original.txt'\n",
    "for i in range(num_classes): #visualise the whole grouping of 45 actions at once as a nested list\n",
    "    print(get_group_labels(location,atog,i))\n",
    "    \n",
    "print(groups_with_subnet)\n",
    "print(groups_without_subnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function definitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Called to load training/testing data \n",
    "def create_dataset(filename, group_number):\n",
    "    # dataset is organized as NxLxD (N = num samples, L temporal length with padding, D feature dimension\n",
    "    # labels is NxY where Y is one hot label vector\n",
    "    \n",
    "    # -3 sends back labels as actions number (0..45) with one hot enconding (000..1..00)\n",
    "    # -2 sends back labels as actions number (0..45) with no one hot enconding (1 or 2 or 45)\n",
    "    # -1 sends back labels as groups number (0..12or26) with one hot encoding (000..1..00)\n",
    "    # 0> sends back labels of only this group with one hot enconding\n",
    "\n",
    "    dataset, labels, lengths = [], [], []\n",
    "    files = read_config(filename)\n",
    "    \n",
    "    if group_number == -3: #standard 45 actions RNN\n",
    "        num_classes = 45\n",
    "    elif group_number == -2: #groupRNN to subnets (testing phase)\n",
    "        #num_classes = 45 #not needed in the function when -2 activated\n",
    "        pass\n",
    "    elif group_number == -1: #groupRNN\n",
    "        num_classes = max(atog) + 1 \n",
    "        #pass\n",
    "    else: #subnets\n",
    "        list_actions = gtoa[group_number] #gives list of actions in current group number\n",
    "        num_classes = len(list_actions)\n",
    "        \n",
    "    for i in files:\n",
    "        \n",
    "        if group_number == -3:  #standard 45 actions RNN\n",
    "            \n",
    "            tmp_data = read_data(directory_dataset + i[0]) #i[0] =  a01s01r01.txt (for eg)\n",
    "            num =  int( i[1] )\n",
    "            tmp_labels = np.transpose(num_to_idx(num, num_classes)) #one hot encoding\n",
    "        \n",
    "        if group_number == -2: #Feed output of groupRNN to pre-trained subnet (testing phase)\n",
    "            \n",
    "            tmp_data = read_data(directory_dataset + i[0]) #i[0] =  a01s01r01.txt (for eg)\n",
    "            tmp_labels = int(i[1]) #will one hot later as first need to dispatch data to relevant subnet using labels\n",
    "        \n",
    "        elif group_number == -1: #Build groupRNN\n",
    "            \n",
    "            tmp_data = read_data(directory_dataset + i[0]) \n",
    "            num =  atog[ int( i[1] )] #only used for one hot encoding in the line below\n",
    "            tmp_labels = np.transpose(num_to_idx(num, num_classes)) \n",
    "            #i[0] =  a01s01r01.txt (for eg)\n",
    "            #i[1] = number between 0 and 44 (or as many classes there is)\n",
    "            #tmp_labels = [0, 0, 1, 0, .. 0] = one-hot encoding of class value\n",
    "            #tmp_data = list with variable len around 300 & each item in the list is a nested list of len 63 (=feat_size)\n",
    "        \n",
    "        else: #Build subnet RNN\n",
    "            \n",
    "            num = int( i[1] ) #original action number\n",
    "            if num in list_actions: #select data only if belongs to group \n",
    "                tmp_data = read_data(directory_dataset + i[0]) #add to dataset if part of the group\n",
    "                tmp_labels = np.transpose(num_to_idx(list_actions.index(num), num_classes)) #add labels as well     \n",
    "            else: continue\n",
    "            \n",
    "        if len(tmp_data)<300: #why 300, is that the longest sequence ? 300 = padding_size btw...\n",
    "            \n",
    "            #records tmp_data initial length before padding\n",
    "            #pads tmp_data with zeros until padding_size (300) so len(tmp_data) = 300 always with len 63 items\n",
    "                \n",
    "            lengths.append(len(tmp_data))\n",
    "            tmp_data.extend([ [0.0] * feat_size ] * (padding_size - len(tmp_data)))  \n",
    "\n",
    "            dataset.append(tmp_data)\n",
    "            labels.append(tmp_labels) \n",
    "        \n",
    "    # all 0..565 lists with item as nested lists of size (300, 26, 1)\n",
    "    return dataset, labels, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Called once when training RNN\n",
    "def batch_generation(data,labels,lengths):\n",
    "    \n",
    "    num_classes = np.size(labels,1) #labels is a one hot encoded numpy array. This returns number of columns (=groups).\n",
    "    \n",
    "    nsamples,_,_ = data.shape\n",
    "\n",
    "    indices = np.arange(nsamples) #np.arange(3) -> array([0, 1, 2])\n",
    "    np.random.shuffle(indices) #shuffle the indices\n",
    "    num_batches = int(np.floor(nsamples/batch_size)) #round to inferior number so = 0 if batch_size bigger than nsamples\n",
    "    not_exact = 0\n",
    "\n",
    "    if nsamples%batch_size != 0: #happens all the time unless nsamples is lucky multiple of batch_size\n",
    "        not_exact = 1\n",
    "    \n",
    "    #declare empty arrays to contain the batches, dimensions are right\n",
    "    batches_data = np.empty(shape=[num_batches+not_exact,batch_size,padding_size,feat_size])\n",
    "    batches_labels = np.empty(shape=[num_batches+not_exact,batch_size,num_classes]) \n",
    "    batches_lengths = np.empty(shape=[num_batches + not_exact, batch_size],dtype=np.int32)\n",
    "\n",
    "    for x in range(num_batches):\n",
    "        batches_data[x, :, :, :] = data[indices[batch_size*x:batch_size*(x+1)], :, :]\n",
    "        batches_labels[x,:,:] = labels[indices[batch_size*x:batch_size*(x+1)], :]\n",
    "        batches_lengths[x,:] = lengths[indices[batch_size*x:batch_size*(x+1)]]\n",
    "\n",
    "    if not_exact > 0:\n",
    "        \n",
    "        to_complete = nsamples%batch_size\n",
    "        \n",
    "        #nsamples is too small, reuse the samples from previous batch, taken randomly to complete this batch\n",
    "        tmp_random = np.random.randint(0,nsamples,batch_size-to_complete) # we complete last batch with random samples\n",
    "        #prints list of indices it will take randomly\n",
    "        \n",
    "        #[num_batches] refers to the last batch that is not complete\n",
    "        tmp_data = data[indices[batch_size*num_batches:batch_size*num_batches+to_complete],:,:]\n",
    "        batches_data[num_batches]=np.concatenate((tmp_data,data[tmp_random,:,:]),axis=0)\n",
    "\n",
    "        tmp_labels = labels[indices[batch_size*num_batches:batch_size*num_batches+to_complete],:]\n",
    "        batches_labels[num_batches] = np.concatenate((tmp_labels,labels[indices[tmp_random],:]))\n",
    "        \n",
    "        tmp_lengths = lengths[indices[batch_size*num_batches:batch_size*num_batches+to_complete]]\n",
    "        batches_lengths[num_batches] = np.concatenate((tmp_lengths,lengths[indices[tmp_random]]))\n",
    "\n",
    "    return batches_data, batches_labels, batches_lengths, num_batches+not_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamicRNN(x, seqlen, weights, biases, keep_prob):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Permuting batch_size and n_steps\n",
    "    x = tf.transpose(x, [1, 0, 2])\n",
    "    # Reshaping to (n_steps*batch_size, n_input)\n",
    "    x = tf.reshape(x, [-1, feat_size])\n",
    "    # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.split(x, seq_max_len, 0) # tf.split(value, num_or_size_splits, axis)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "    \n",
    "    #Operator adding dropout to inputs and outputs of the given cell.\n",
    "    lstm_cell_dropout = rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob = keep_prob) \n",
    "    \n",
    "    #Added to have 2 layers LSTM, not used here.\n",
    "    layers = 1\n",
    "    final_cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell_dropout] * layers)\n",
    "\n",
    "    # Get lstm cell output, providing 'sequence_length' will perform dynamic calculation.\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(final_cell, x, dtype=tf.float32, sequence_length=seqlen)\n",
    "    \n",
    "    # When performing dynamic calculation, we must retrieve the last\n",
    "    # dynamically computed output, i.e, if a sequence length is 10, we need\n",
    "    # to retrieve the 10th output.\n",
    "    # However TensorFlow doesn't support advanced indexing yet, so we build\n",
    "    # a custom op that for each sample in batch size, get its length and\n",
    "    # get the corresponding relevant output.\n",
    "\n",
    "    # 'outputs' is a list of output at every timestep, we pack them in a Tensor\n",
    "    # and change back dimension to [batch_size, n_step, n_input]\n",
    "    outputs = tf.stack(outputs)\n",
    "    outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "\n",
    "    # Hack to build the indexing and retrieve the right output.\n",
    "    batch_size = tf.shape(outputs)[0]\n",
    "    # Start indices for each sample\n",
    "    index = tf.range(0, batch_size) * seq_max_len + (seqlen - 1)\n",
    "    # Indexing\n",
    "    outputs = tf.gather(tf.reshape(outputs, [-1, n_hidden]), index)\n",
    "\n",
    "    # Linear activation, using outputs computed above\n",
    "    return tf.matmul(outputs, weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doMyRNN(family, group_number):\n",
    "    \n",
    "    # group_number = \n",
    "    # -3 training stanard 45 actions RNN\n",
    "    # -2 never\n",
    "    # -1 training groupRNN\n",
    "    # 0> training subnet\n",
    "\n",
    "    # ==========\n",
    "    #   MODEL\n",
    "    # ==========\n",
    "    print('in MODEL')\n",
    "\n",
    "    tf.reset_default_graph() #Clear computational graph to prevent error\n",
    "    \n",
    "    # Load training and testing data\n",
    "    train_data, train_labels, train_lengths = create_dataset(file_training, group_number)\n",
    "    test_data, test_labels, test_lengths = create_dataset(file_testing, group_number)\n",
    "    \n",
    "    #cast to numpy array\n",
    "    train_data = np.asarray(train_data)\n",
    "    train_labels = np.asarray(train_labels)\n",
    "    train_lengths = np.asarray(train_lengths,dtype=np.int32)\n",
    "    test_data = np.asarray(test_data)\n",
    "    test_labels = np.asarray(test_labels)\n",
    "    test_lengths = np.asarray(test_lengths)\n",
    "    \n",
    "    #Set validation/training split\n",
    "    tr_data, val_data, tr_labels, val_labels, tr_lengths, val_lengths = train_test_split(train_data, train_labels, train_lengths, \n",
    "                                                                                            test_size=0.2, random_state=42, stratify=train_labels)\n",
    "\n",
    "    (samples, rows, row_size) = train_data.shape\n",
    "\n",
    "    n_classes = np.size(train_labels,1)\n",
    "\n",
    "    # tf Graph input\n",
    "    x = tf.placeholder(\"float\", [None, seq_max_len, feat_size], name=\"x\")\n",
    "    y = tf.placeholder(\"float\", [None, n_classes], name=\"y\")\n",
    "\n",
    "    # A placeholder for indicating each sequence length\n",
    "    seqlen = tf.placeholder(tf.int32, [None], name=\"seqlen\")\n",
    "\n",
    "    # Define weights\n",
    "    weights = {\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "    # ==========\n",
    "    #   LAUNCH\n",
    "    # ==========\n",
    "    print('in LAUNCH')\n",
    "\n",
    "    pred = dynamicRNN(x, seqlen, weights, biases, keep_prob)\n",
    "\n",
    "    # Evaluate model \n",
    "    prediction = tf.argmax(pred,axis=1, name=\"prediction\") # for each prediction, keep class with highest level of confidence (tests X classes)\n",
    "    correct_pred = tf.equal(prediction, tf.argmax(y,1)) #output 0 & 1 vector, y is supposed to have true labels\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32)) #mean of above\n",
    "\n",
    "    #Create a saver object which will save all the variables\n",
    "    #saver = tf.train.Saver()\n",
    "\n",
    "    # ============\n",
    "    #   OPTIMIZE\n",
    "    # ============\n",
    "    print('in OPTIMIZE \\n')\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = pred, labels = y))\n",
    "    # optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    # optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    #Create a saver object which will save all the variables\n",
    "    saver = tf.train.Saver() \n",
    "    if group_number == -3 :\n",
    "        saved_path = '/home/nathan/Documents/FYP_code/LSTM1_guillermo/grouping_0/'\n",
    "    elif group_number == -1 :\n",
    "        saved_path = directory + 'saved_sessions/group_RNN/' + family \n",
    "    else:\n",
    "        saved_path = directory + 'saved_sessions/subnets/' + family + '/subnet_' + str(group_number)\n",
    "\n",
    "    # Launch the graph\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        sess.run(init)\n",
    "\n",
    "        max_acc = 0.0;\n",
    "        max_epoch = 0;\n",
    "        best_labels = []\n",
    "\n",
    "        # Keep training until reach max iterations\n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            (batch_x, batch_y, batch_seqlen, n_batches) = batch_generation(tr_data, tr_labels, tr_lengths)\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            for i in range(n_batches):\n",
    "\n",
    "                # Run optimization op (backprop)\n",
    "                sess.run(optimizer, feed_dict={x: batch_x[i,:,:,:], y: batch_y[i,:,:],\n",
    "                                               seqlen: batch_seqlen[i,:], keep_prob: p_dropout})\n",
    "\n",
    "            # Test accuracy on this epoch    \n",
    "            validation_acc = sess.run(accuracy, feed_dict={x: val_data, y: val_labels, seqlen: val_lengths, keep_prob: 1.0})\n",
    "\n",
    "            #Save best validation results\n",
    "            if validation_acc > max_acc: \n",
    "                \n",
    "                max_acc = validation_acc; max_epoch = epoch\n",
    "                    \n",
    "                #Save best model of the NN\n",
    "                saver.save(sess, saved_path)\n",
    "                print('Saved subnet at epoch ' + str(epoch) + ' at ' + saved_path)\n",
    "\n",
    "            print('Epoch {:2d} validation accuracy {:3.1f}% in {:3.1f} seconds'.format(epoch, 100 * validation_acc, time.time() - start))\n",
    "            print('max_acc {:3.1f}% at epoch {:2d} \\n'.format(max_acc*100, max_epoch))\n",
    "            \n",
    "            #Stop training when accuracy is maximum\n",
    "            if max_acc == 1: \n",
    "                print('Reached 100% accuracy -> exit training \\n')\n",
    "                break\n",
    "             \n",
    "        print (\"Optimization Finished! \\n\")\n",
    "\n",
    "        # ToDo\n",
    "        # Get training accuracy and plot it along validation accuracy and test accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display confusion matrix\n",
    "def confusion_mtx(true, pred, name_labels):\n",
    "    \n",
    "    num_classes = len(name_labels)\n",
    "    cm = confusion_matrix(true, pred)\n",
    "\n",
    "    fig = plt.figure(figsize=(10,10), dpi=100)\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(cm)\n",
    "    fig.colorbar(cax)\n",
    "    plt.xticks(range(num_classes), name_labels, rotation=90)\n",
    "    plt.yticks(range(num_classes), name_labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "    \n",
    "    #TODO\n",
    "    #Adjust the matrix scale (0 to 45 to the relevant scaling 0 to 100%)\n",
    "    #Have a checkerboard figure to track classes easily\n",
    "    #Have white background for better error reading for colorbar\n",
    "    #Rotate 45 the vertical axis\n",
    "    #Write number of successful matches inside the box (especially good for subnets)\n",
    "\n",
    "#Display confidence level bar histogram \n",
    "def confidence_probability_g (scores, pred_class, true_class, g_labels):\n",
    "    \n",
    "    format_scores = scores - np.amin(scores)\n",
    "    format_scores = format_scores / np.amax(format_scores)\n",
    "    \n",
    "    #y = scores\n",
    "    y = format_scores #probabilities made from normalising on this 1 score (not on the 569 scores)\n",
    "    x = range(len(scores)) #as much scores as there is in y (45)\n",
    "    \n",
    "    plt.figure(figsize=(20,4))\n",
    "    \n",
    "    barlist = plt.bar(x,y, width=0.7)\n",
    "\n",
    "    import randomcolor\n",
    "    colors = randomcolor.RandomColor().generate(count=num_classes) #as many group or actions    \n",
    "    for i in range(len(barlist)): barlist[i].set_color(colors[i])\n",
    "    \n",
    "    plt.title('Predicted class ' + str(pred_class) + ' ' + g_labels[pred_class] \n",
    "                 + ' for class ' + str(true_class) + ' ' + g_labels[true_class],\n",
    "                 fontsize=18)\n",
    "    \n",
    "    plt.xticks(x, g_labels, rotation=90, fontsize=18)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    #ToDo\n",
    "    # Be able to know the name of sample seq that fails \n",
    "    \n",
    "def compute_accuracy(groups, pred_labels, test_labels):\n",
    "    \n",
    "    indices = [index for index, value in enumerate(pred_labels) if value in groups]\n",
    "    p, t = pred_labels[indices], test_labels[indices]\n",
    "    group_t = [atog[value] for value in t] #convert t labels from action to group number for comparison with p\n",
    "    \n",
    "    correct_pred = np.equal(p, group_t)\n",
    "    \n",
    "    return correct_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def useSubnet(group_number, pred_labels, test_data, test_labels, test_lenghts):\n",
    "    \n",
    "    #Alreaedy done in create dataset by using: (sub_test_data, sub_test_labels, sub_test_lengths) = create_dataset(file_testing, group_number)\n",
    "    #Select relevant subset of data, labels and lengths to use on subnet\n",
    "    indices = [index for index, value in enumerate(pred_labels) if value == group_number]\n",
    "    (sub_test_data, sub_test_labels, sub_test_lengths) = test_data[indices], test_labels[indices], test_lengths[indices]\n",
    "    #sub_test_labels is the subnet truth label to compare to subnet output\n",
    "    \n",
    "    tf.reset_default_graph() #Clear computational graph to prevent error\n",
    "    \n",
    "    #Load pre-trained subnet\n",
    "    sess=tf.Session()    \n",
    "    location = directory + 'saved_sessions/subnets/' + family + '/subnet_' + str(group_number)\n",
    "    saver = tf.train.import_meta_graph(location + \".meta\")\n",
    "    saver.restore(sess, location)\n",
    "    graph = tf.get_default_graph()\n",
    "    \n",
    "    #Import variables\n",
    "    x = graph.get_tensor_by_name(\"x:0\")\n",
    "    y = graph.get_tensor_by_name(\"y:0\")\n",
    "    seqlen = graph.get_tensor_by_name(\"seqlen:0\")\n",
    "    keep_prob = graph.get_tensor_by_name(\"keep_prob:0\")\n",
    "    \n",
    "    #Pass test samples through RNN\n",
    "    feed_dict={x: sub_test_data, seqlen: sub_test_lengths, keep_prob: 1.0} \n",
    "    prediction = graph.get_tensor_by_name(\"prediction:0\")\n",
    "    sub_pred_labels = sess.run(prediction, feed_dict) \n",
    "    \n",
    "    #Convert output back to original action number \n",
    "    list_actions = gtoa[group_number] \n",
    "    original_sub_pred_labels = [list_actions[i] for i in sub_pred_labels] \n",
    "    #assumes number of subnet output is same order as actions in gtoa...\n",
    "    \n",
    "    #Compute results\n",
    "    correct_pred = np.equal(original_sub_pred_labels, sub_test_labels) #output 0 & 1 vector\n",
    "    accuracy_with = np.mean(correct_pred)\n",
    "    \n",
    "    print(\"Accuracy subnet %i %s is %.2f %%\" % (group_number, g_labels[group_number], float(100*accuracy_with)) )\n",
    "    #onfusion_mtx(original_sub_pred_labels, sub_test_labels, g_labels[group_number])\n",
    "    \n",
    "    return correct_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def useGroupRNN():\n",
    "    \n",
    "    (test_data, test_labels, test_lengths) = create_dataset(file_testing, -1)\n",
    "    true_labels = np.asarray( [ np.where(r==1)[0][0] for r in test_labels ], dtype=np.int64) #one hot to action number\n",
    "    \n",
    "    tf.reset_default_graph() #Clear computational graph to prevent error\n",
    "    \n",
    "    #Load pre-trained subnet\n",
    "    sess=tf.Session()    \n",
    "    location = directory + 'saved_sessions/group_RNN/' + family \n",
    "    saver = tf.train.import_meta_graph(location + \".meta\")\n",
    "    saver.restore(sess, location)\n",
    "    graph = tf.get_default_graph()\n",
    "    \n",
    "    #Import variables\n",
    "    x = graph.get_tensor_by_name(\"x:0\")\n",
    "    y = graph.get_tensor_by_name(\"y:0\")\n",
    "    seqlen = graph.get_tensor_by_name(\"seqlen:0\")\n",
    "    keep_prob = graph.get_tensor_by_name(\"keep_prob:0\")\n",
    "    \n",
    "    #Pass test samples through RNN\n",
    "    feed_dict={x: test_data, seqlen: test_lengths, keep_prob: 1.0} \n",
    "    prediction = graph.get_tensor_by_name(\"prediction:0\")\n",
    "    pred_labels = sess.run(prediction, feed_dict) \n",
    "    \n",
    "    #Compute results\n",
    "    correct_pred = np.equal(pred_labels, true_labels) #output 0 & 1 vector\n",
    "    accuracy = np.mean(correct_pred)\n",
    "    print(\"Accuracy groupRNN %s with %i classes is %.2f %%\" % (family, num_classes, float(100*accuracy))) \n",
    "    confusion_mtx(true_labels, pred_labels, g_labels)\n",
    "    \n",
    "    return pred_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execution**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standard 45 actions RNN**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train RNN & save\n",
    "doMyRNN(family, -3) #family argument ignored if you pass -3 OR pass family no_grouping... chepa\n",
    "\n",
    "# Import and test\n",
    "#....\n",
    "\n",
    "#Calculate accuracy\n",
    "correct_pred = np.equal(true_labels, pred_labels) #output 0 & 1 vector\n",
    "acc = np.mean(correct_pred) #mean of above\n",
    "\n",
    "# Extract label names from text file\n",
    "with open(directory + 'labels/name_of_labels_original.txt') as f:\n",
    "    name_labels = [word for line in f for word in line.split()]\n",
    "    \n",
    "print('Accuracy %.2f %%' % float(100*acc))  \n",
    "confusion_mtx(true_labels, pred_labels, name_labels, 45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Group RNN **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train group RNN on classifying the object group\n",
    "doMyRNN(family, -1) #-1 as this is a main group net (not subnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyze group RNN results\n",
    "pred_labels = useGroupRNN()\n",
    "\n",
    "#Save output to feed to subnet\n",
    "location = directory + 'results/group_RNN/' + family + '/'\n",
    "np.save(location + 'pred_labels.npy', pred_labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subnets RNN **"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#1. Train all subnets with a for loop and save them all\n",
    "#2. Import each as pre-trained model\n",
    "#3. Gather relevant output of groupRNN \n",
    "#4. Feed it to subnet\n",
    "#5. Get accuracy of each subnet and overall accuracy, including the outputs that did not get a subnet (1 action groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train subnets independently of group RNN \n",
    "\n",
    "#Build a subnet for all groups\n",
    "n_epochs = 100 #smaller training for smaller nets\n",
    "for group_number in groups_with_subnet:\n",
    "\n",
    "    num_classes = gtoa[group_number] #number of classes\n",
    "    \n",
    "    #Trained subnet are saved in doMyRNN\n",
    "    doMyRNN(family, group_number)\n",
    "    #Subnets are just like the orginal RNN but with a selected number of training samples from training.txt and testing.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Evaluate the subnet after its training\n",
    "    #acc = np.mean(np.equal(true_labels, pred_labels))\n",
    "    #print('Group %i \"%s\" - %s - accuracy: %.2f %%' % (group_number, g_labels[group_number], group_sub_labels, float(100*acc)))\n",
    "    #print(pred_labels); print(true_labels); print(np.equal(true_labels, pred_labels))\n",
    "    #location = directory + 'labels/name_of_labels_original.txt'\n",
    "    #group_sub_labels = get_group_labels(location, atog, group_number) #labels of classes (inside this group)\n",
    "    #confusion_mtx(true_labels, pred_labels, group_sub_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final group+subnet RNN **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of groupRNN only on 1 and multi action groups: 0.00% and 70.12% \n"
     ]
    }
   ],
   "source": [
    "#Load data & Compute groupRNN accuracy on 1 action and multi action groups\n",
    "\n",
    "#Obtain all original data\n",
    "(test_data, test_labels, test_lengths) = create_dataset(file_testing, -2)\n",
    "test_data, test_labels, test_lengths = np.asarray(test_data), np.asarray(test_labels), np.asarray(test_lengths) \n",
    "\n",
    "#Link groupRNN output to subnets input using pred_labels\n",
    "location = directory + 'results/group_RNN/' + family + '/'\n",
    "pred_labels = np.load(location + 'pred_labels.npy') \n",
    "\n",
    "if not groups_without_subnet: #means list is empty (no 1 action groups)\n",
    "    correct_pred_without, accuracy_without = [], 0\n",
    "else: #1 action groups exist\n",
    "    correct_pred_without = compute_accuracy(groups_without_subnet, pred_labels, test_labels)\n",
    "    accuracy_without = np.mean(correct_pred_without) \n",
    "correct_pred_with = compute_accuracy(groups_with_subnet, pred_labels, test_labels)\n",
    "accuracy_with = np.mean(correct_pred_with) \n",
    "print(\"Accuracy of groupRNN only on 1 and multi action groups: %.2f%% and %.2f%% \" % (float(100*accuracy_without), float(100*accuracy_with)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyze overall architecture group+subnet results\n",
    "\n",
    "#Helps to know what is left to compute\n",
    "print('Now using subnets for groups ' + str(groups_with_subnet))\n",
    "\n",
    "#Assume groupRNN 100% pred and evaluate subnet results\n",
    "correct_pred_with = []\n",
    "group_test_labels = [atog[value] for value in test_labels] #convert to group number for comparison\n",
    "for group_number in groups_with_subnet:\n",
    "    correct_pred = useSubnet(group_number, group_test_labels, test_data, test_labels, test_lengths)\n",
    "    correct_pred_with = np.append(correct_pred_with, correct_pred) #accumulate results\n",
    "correct_pred_with_accuracy = np.mean(correct_pred_with)\n",
    "print('\\n Assuming groupRNN is 100%% perfect, accuracy of subnets only: %.2f %% \\n' % float(100*correct_pred_with_accuracy))\n",
    "\n",
    "#Pass groupRNN output through subnets and collect results\n",
    "correct_pred_with = []\n",
    "for group_number in groups_with_subnet:\n",
    "    correct_pred = useSubnet(group_number, pred_labels, test_data, test_labels, test_lengths)\n",
    "    correct_pred_with = np.append(correct_pred_with, correct_pred) #accumulate results   \n",
    "correct_pred_with_accuracy = np.mean(correct_pred_with)\n",
    "print('Using groupRNN output, accuracy of subnets only: %.2f %%' % float(100*correct_pred_with_accuracy)) \n",
    "#above should be same as below if no 1 action groups\n",
    "\n",
    "#Final result\n",
    "final_pred = np.append(correct_pred_without, correct_pred_with) #add previously known results (groups with 1 action)\n",
    "final_accuracy = np.mean(final_pred)\n",
    "print('Final group+subnet accuracy: %.2f %%' % float(100*final_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
