{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# based on https://danijar.com/variable-sequence-lengths-in-tensorflow/\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import functools\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import re #regexp\n",
    "\n",
    "from tensorflow.python.ops import rnn_cell, rnn\n",
    "from sklearn.metrics import confusion_matrix #compute confusion_matrix\n",
    "from matplotlib import pyplot as plt #display confusion_matrix\n",
    "\n",
    "family = 'Object' \n",
    "\n",
    "directory = '/home/nathan/Documents/FYP_code/LSTM1_guillermo/'\n",
    "directory_dataset = directory + 'data_nathan/'\n",
    "\n",
    "file_training = \"training.txt\"\n",
    "file_testing = \"testing.txt\" \n",
    "\n",
    "feat_size = 63 #21 joints * 3 dimensions (xyz)\n",
    "\n",
    "batch_size = 20\n",
    "padding_size = 300\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.003\n",
    "n_epochs = 100\n",
    "\n",
    "# Network Parameters\n",
    "seq_max_len = padding_size # Sequence max length\n",
    "n_hidden = 100 # hidden layer num of features\n",
    "max_seq_l = 120\n",
    "\n",
    "p_dropout = 0.2 #1 #0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract action-to-group dictionary & group labels\n",
    "def action_to_group (location, family):\n",
    "\n",
    "    with open(location, 'r') as f:\n",
    "        pattern = '_(\\w+)' #for Object by default\n",
    "        if family == 'Motion': pattern = '(\\w+)_' #add patterns like this as a list\n",
    "        regexp = re.compile(pattern)\n",
    "        get = re.findall( regexp, f.read() )\n",
    "\n",
    "    atog = list() #action to group dictionary \n",
    "    g_labels = list() \n",
    "    for i in get:\n",
    "        #check if group already exists\n",
    "        if i not in g_labels: #extract group number as index of word in submitted & add it to action group dictionary\n",
    "            g_labels.append(i)\n",
    "        atog.append( g_labels.index(i) ) \n",
    "\n",
    "    #num_g = len(g_labels) FYI\n",
    "    \n",
    "    return (atog, g_labels) \n",
    "\n",
    "def read_data(filename):\n",
    "    # Reads file containing features and returns features indexed by time\n",
    "    x = []\n",
    "    tmp_length = 0\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            numbers_str = line.split()\n",
    "            nums_float = [float(a) for a in numbers_str]\n",
    "            x.append(nums_float)\n",
    "            tmp_length =tmp_length+1\n",
    "            # print(len(x))\n",
    "    # x.extend([[0.0]*feat_size]*(padding_size-len(x)+1))\n",
    "    f.close() #necessary ? supposed to be automatic\n",
    "    tmp_val = np.min([tmp_length-1,max_seq_l])\n",
    "    #print(tmp_val)\n",
    "    return x[1:]  # ignore de first line (num of frames)\n",
    "\n",
    "def read_config(filename):\n",
    "    # Reads config file and returns filenames and class label\n",
    "    x = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            line_split = line.split()\n",
    "            x.append(line_split)\n",
    "    f.close()\n",
    "    return x\n",
    "\n",
    "# one hot encoding\n",
    "def num_to_idx(num, num_classes):\n",
    "    vec = np.zeros( shape=num_classes, dtype=np.float) #hardcode here\n",
    "    vec[num] = 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Called once to load training/testing data in MODEL\n",
    "def create_dataset(filename, atog, group_number):\n",
    "    # dataset is organized as NxLxD (N = num samples, L temporal length with padding, D feature dimension\n",
    "    # labels is NxY where Y is one hot label vector\n",
    "\n",
    "    dataset, labels, lengths = [], [], []\n",
    "    files = read_config(filename)\n",
    "    \n",
    "    if group_number == -2: #groupRNN to subnets\n",
    "        pass\n",
    "    elif group_number == -1: #main RNN\n",
    "        num_classes = max(atog) + 1 # as labelled 0 to 25 but want 26 as number of classes\n",
    "    else: #subnets\n",
    "        gtoa = group_to_action(atog) \n",
    "        list_actions = gtoa[group_number] #gives list of actions in current group number\n",
    "        num_classes = len(list_actions)\n",
    "        \n",
    "    for i in files:\n",
    "        \n",
    "        if group_number == -2: #Feed output of groupRNN to pre-trained subnet\n",
    "            \n",
    "            tmp_data = read_data(directory_dataset + i[0]) #i[0] =  a01s01r01.txt (for eg)\n",
    "            tmp_labels = int(i[1]) #will one hot out of here as first need to dispatch data to relevant subnet using labels\n",
    "        \n",
    "        elif group_number == -1: #Build group main RNN\n",
    "            \n",
    "            tmp_data = read_data(directory_dataset + i[0]) \n",
    "            num =  atog[ int( i[1] )] #only used for one hot encoding in the line below\n",
    "            tmp_labels = np.transpose(num_to_idx(num, num_classes)) \n",
    "            #i[0] =  a01s01r01.txt (for eg)\n",
    "            #i[1] = number between 0 and 44 (or as many classes there is)\n",
    "            #tmp_labels = [0, 0, 1, 0, .. 0] = one-hot encoding of class value\n",
    "            #tmp_data = list with variable len around 300 & each item in the list is a nested list of len 63 (=feat_size)\n",
    "        \n",
    "        else: #Build action subnet RNN \n",
    "            \n",
    "            num = int( i[1] ) #original action number\n",
    "            if num in list_actions: #select data only if belongs to group \n",
    "                tmp_data = read_data(directory_dataset + i[0]) #add to dataset if part of the group\n",
    "                tmp_labels = np.transpose(num_to_idx(list_actions.index(num), num_classes)) #add labels as well     \n",
    "            else: continue\n",
    "            \n",
    "        if len(tmp_data)<300: #why 300, is that the longest sequence ? 300 = padding_size btw...\n",
    "            \n",
    "            #records tmp_data initial length before padding\n",
    "            #pads tmp_data with zeros until padding_size (300) so len(tmp_data) = 300 always with len 63 items\n",
    "                \n",
    "            lengths.append(len(tmp_data))\n",
    "            tmp_data.extend([ [0.0] * feat_size ] * (padding_size - len(tmp_data)))  \n",
    "\n",
    "            dataset.append(tmp_data)\n",
    "            labels.append(tmp_labels) \n",
    "        \n",
    "    # all 0..565 lists with item as nested lists of size (300, 26, 1)\n",
    "    return dataset, labels, lengths\n",
    "\n",
    "#Called once when training RNN\n",
    "def batch_generation(data,labels,lengths):\n",
    "    \n",
    "    num_classes = np.size(labels,1) #labels is a one hot encoded numpy array. This returns number of columns (=groups).\n",
    "    \n",
    "    nsamples,_,_ = data.shape\n",
    "\n",
    "    indices = np.arange(nsamples)\n",
    "    np.random.shuffle(indices)\n",
    "    num_batches = int(np.floor(nsamples/batch_size))\n",
    "    not_exact = 0\n",
    "\n",
    "    if nsamples%batch_size != 0:\n",
    "        not_exact = 1\n",
    "\n",
    "    batches_data = np.empty(shape=[num_batches+not_exact,batch_size,padding_size,feat_size])\n",
    "    batches_labels = np.empty(shape=[num_batches+not_exact,batch_size,num_classes]) #hardcode here with num_classes\n",
    "    batches_lengths = np.empty(shape=[num_batches + not_exact, batch_size],dtype=np.int32)\n",
    "\n",
    "    for x in range(num_batches):\n",
    "        batches_data[x, :, :, :] = data[indices[batch_size*x:batch_size*(x+1)], :, :]\n",
    "        batches_labels[x,:,:] = labels[indices[batch_size*x:batch_size*(x+1)], :]\n",
    "        batches_lengths[x,:] = lengths[indices[batch_size*x:batch_size*(x+1)]]\n",
    "\n",
    "    if not_exact > 0:\n",
    "        to_complete = nsamples%batch_size\n",
    "        tmp_data = data[indices[batch_size*num_batches:batch_size*num_batches+to_complete],:,:]\n",
    "        tmp_random = np.random.randint(0,nsamples-to_complete,batch_size-to_complete) # we complete last batch with random samples\n",
    "\n",
    "        batches_data[num_batches]=np.concatenate((tmp_data,data[tmp_random,:,:]),axis=0)\n",
    "\n",
    "        tmp_labels = labels[indices[batch_size*num_batches:batch_size*num_batches+to_complete],:]\n",
    "        batches_labels[num_batches] = np.concatenate((tmp_labels,labels[indices[tmp_random],:]))\n",
    "        tmp_lengths = lengths[indices[batch_size*num_batches:batch_size*num_batches+to_complete]]\n",
    "        batches_lengths[num_batches] = np.concatenate((tmp_lengths,lengths[indices[tmp_random]]))\n",
    "\n",
    "    return batches_data, batches_labels, batches_lengths, num_batches+not_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamicRNN(x, seqlen, weights, biases, keep_prob):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Permuting batch_size and n_steps\n",
    "    x = tf.transpose(x, [1, 0, 2])\n",
    "    # Reshaping to (n_steps*batch_size, n_input)\n",
    "    x = tf.reshape(x, [-1, feat_size])\n",
    "    # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.split(x, seq_max_len, 0) # tf.split(value, num_or_size_splits, axis)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "    \n",
    "    #Operator adding dropout to inputs and outputs of the given cell.\n",
    "    lstm = rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob) \n",
    "    \n",
    "    #Added to have 2 layers LSTM, not used here.\n",
    "    layers = 1\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * layers)\n",
    "\n",
    "    # Get lstm cell output, providing 'sequence_length' will perform dynamic calculation.\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cell, x, dtype=tf.float32, sequence_length=seqlen)\n",
    "    \n",
    "    # When performing dynamic calculation, we must retrieve the last\n",
    "    # dynamically computed output, i.e, if a sequence length is 10, we need\n",
    "    # to retrieve the 10th output.\n",
    "    # However TensorFlow doesn't support advanced indexing yet, so we build\n",
    "    # a custom op that for each sample in batch size, get its length and\n",
    "    # get the corresponding relevant output.\n",
    "\n",
    "    # 'outputs' is a list of output at every timestep, we pack them in a Tensor\n",
    "    # and change back dimension to [batch_size, n_step, n_input]\n",
    "    outputs = tf.stack(outputs)\n",
    "    outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "\n",
    "    # Hack to build the indexing and retrieve the right output.\n",
    "    batch_size = tf.shape(outputs)[0]\n",
    "    # Start indices for each sample\n",
    "    index = tf.range(0, batch_size) * seq_max_len + (seqlen - 1)\n",
    "    # Indexing\n",
    "    outputs = tf.gather(tf.reshape(outputs, [-1, n_hidden]), index)\n",
    "\n",
    "    # Linear activation, using outputs computed above\n",
    "    return tf.matmul(outputs, weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doMyRNN(location, family, group_number):\n",
    "\n",
    "    # ==========\n",
    "    #   MODEL\n",
    "    # ==========\n",
    "    print('in MODEL')\n",
    "\n",
    "    tf.reset_default_graph() #Clear computational graph to prevent error\n",
    "\n",
    "    # Load training and testing data\n",
    "    (atog, _) = action_to_group(location, family)\n",
    "    train_data, train_labels, train_lengths = create_dataset(file_training, atog, group_number)\n",
    "    test_data, test_labels, test_lengths = create_dataset(file_testing, atog, group_number)\n",
    "\n",
    "    #cast to numpy array\n",
    "    train_data = np.asarray(train_data)\n",
    "    train_labels = np.asarray(train_labels)\n",
    "    train_lengths = np.asarray(train_lengths,dtype=np.int32)\n",
    "    test_data = np.asarray(test_data)\n",
    "    test_labels = np.asarray(test_labels)\n",
    "    test_lengths = np.asarray(test_lengths)\n",
    "\n",
    "    (samples, rows, row_size) = train_data.shape\n",
    "\n",
    "    n_classes = np.size(train_labels,1)\n",
    "\n",
    "    # tf Graph input\n",
    "    x = tf.placeholder(\"float\", [None, seq_max_len, feat_size], name=\"x\")\n",
    "    y = tf.placeholder(\"float\", [None, n_classes], name=\"y\")\n",
    "\n",
    "    # A placeholder for indicating each sequence length\n",
    "    seqlen = tf.placeholder(tf.int32, [None], name=\"seqlen\")\n",
    "\n",
    "    # Define weights\n",
    "    weights = {\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "    # ==========\n",
    "    #   LAUNCH\n",
    "    # ==========\n",
    "    print('in LAUNCH')\n",
    "\n",
    "    pred = dynamicRNN(x, seqlen, weights, biases, keep_prob)\n",
    "\n",
    "    # Evaluate model \n",
    "    prediction = tf.argmax(pred,axis=1, name=\"prediction\") # for each prediction, keep class with highest level of confidence (tests X classes)\n",
    "    correct_pred = tf.equal(prediction, tf.argmax(y,1)) #output 0 & 1 vector, y is supposed to have true labels\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32)) #mean of above\n",
    "\n",
    "    #Create a saver object which will save all the variables\n",
    "    #saver = tf.train.Saver()\n",
    "\n",
    "    # ============\n",
    "    #   OPTIMIZE\n",
    "    # ============\n",
    "    print('in OPTIMIZE')\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = pred, labels = y))\n",
    "    # optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    # optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Launch the graph\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        sess.run(init)\n",
    "\n",
    "        max_acc = 0.0;\n",
    "        max_epoch = 0;\n",
    "        best_labels = []\n",
    "\n",
    "        # Keep training until reach max iterations\n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "            (batch_x, batch_y, batch_seqlen, n_batches) = batch_generation(train_data, train_labels, train_lengths)\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            for i in range(n_batches):\n",
    "\n",
    "                # Run optimization op (backprop)\n",
    "                sess.run(optimizer, feed_dict={x: batch_x[i,:,:,:], y: batch_y[i,:,:],\n",
    "                                               seqlen: batch_seqlen[i,:], keep_prob: p_dropout})\n",
    "\n",
    "            # Test accuracy on this epoch    \n",
    "            test_acc = sess.run(accuracy, feed_dict={x: test_data, y: test_labels, seqlen: test_lengths, keep_prob: 1.0})\n",
    "\n",
    "            #Save best testing results\n",
    "            if test_acc > max_acc: \n",
    "                \n",
    "                #Save the best accuracy and predictions\n",
    "                max_acc = test_acc; max_epoch = epoch\n",
    "                #gives the confidence score of every class at output along columns (dim 1)\n",
    "                labels = pred.eval(feed_dict={x: test_data, y: test_labels, seqlen: test_lengths, keep_prob: 1.0})\n",
    "                # pred_labels = tf.argmax(labels, 1) OR\n",
    "                pred_labels = prediction.eval(feed_dict={x: test_data, y: test_labels, seqlen: test_lengths, keep_prob: 1.0})\n",
    "                #test_labels is one_hot. Want 1-D like best_labels\n",
    "                true_labels = np.asarray( [ np.where(r==1)[0][0] for r in test_labels ], dtype=np.int64)\n",
    "\n",
    "                #Save best  \n",
    "                saver = tf.train.Saver() #Create a saver object which will save all the variables\n",
    "                location = directory + 'saved_sessions/subnets/' + family + '/subnet_' + str(group_number)\n",
    "                saver.save(sess, location)\n",
    "                print('Saved subnet at epoch ' + str(epoch) + ' at ' + location)\n",
    "\n",
    "            print('Epoch {:2d} accuracy {:3.1f}% in {:3.1f} seconds'.format(epoch, 100 * test_acc, time.time() - start))\n",
    "            print('max_acc {:3.1f}% at epoch {:2d} \\n'.format(max_acc*100, max_epoch))\n",
    "            \n",
    "            #Stop training when accuracy is maximum\n",
    "            if max_acc == 1: \n",
    "                print('Reached 100% accuracy -> exit training \\n')\n",
    "                break\n",
    "             \n",
    "        print (\"Optimization Finished!\")\n",
    "\n",
    "        return (labels, pred_labels, true_labels)\n",
    "\n",
    "        # ToDo\n",
    "        # Get training accuracy and plot it along test accuracy \n",
    "        # Add a validation set accuracy to train without overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display confusion matrix\n",
    "def confusion_mtx(true, pred, name_labels):\n",
    "    \n",
    "    num_classes = len(name_labels)\n",
    "    cm = confusion_matrix(true, pred)\n",
    "\n",
    "    fig = plt.figure(figsize=(10,10), dpi=100)\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(cm)\n",
    "    fig.colorbar(cax)\n",
    "    plt.xticks(range(num_classes), name_labels, rotation=90)\n",
    "    plt.yticks(range(num_classes), name_labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "    \n",
    "    #TODO\n",
    "    #Adjust the matrix scale (0 to 45 to the relevant scaling 0 to 100%)\n",
    "    #Have a checkerboard figure to track classes easily\n",
    "    #Have white background for better error reading for colorbar\n",
    "    #Rotate 45 the vertical axis\n",
    "    #Write number of successful matches inside the box (especially good for subnets)\n",
    "\n",
    "#Display confidence level bar histogram \n",
    "def confidence_probability_g (scores, pred_class, true_class, g_labels):\n",
    "    \n",
    "    format_scores = scores - np.amin(scores)\n",
    "    format_scores = format_scores / np.amax(format_scores)\n",
    "    \n",
    "    #y = scores\n",
    "    y = format_scores #probabilities made from normalising on this 1 score (not on the 569 scores)\n",
    "    x = range(len(scores)) #as much scores as there is in y (45)\n",
    "    \n",
    "    plt.figure(figsize=(20,4))\n",
    "    \n",
    "    barlist = plt.bar(x,y, width=0.7)\n",
    "\n",
    "    import randomcolor\n",
    "    colors = randomcolor.RandomColor().generate(count=len(g_labels)) #as many group or actions    \n",
    "    for i in range(len(barlist)): barlist[i].set_color(colors[i])\n",
    "    \n",
    "    plt.title('Predicted class ' + str(pred_class) + ' ' + g_labels[pred_class] \n",
    "                 + ' for class ' + str(true_class) + ' ' + g_labels[true_class],\n",
    "                 fontsize=18)\n",
    "    \n",
    "    plt.xticks(x, g_labels, rotation=90, fontsize=18)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    #ToDo\n",
    "    # Be able to know the name of sample seq that fails "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_labels(group_number):\n",
    "    with open(directory + 'labels/name_of_labels_original.txt') as f:\n",
    "        all_45_action_labels = [word for line in f for word in line.split()]\n",
    "    action_labels_inside_this_group = [all_45_action_labels[i] for i in range(len(atog)) if atog[i] == group_number]\n",
    "    return action_labels_inside_this_group\n",
    "\n",
    "def group_to_action(atog):\n",
    "    gtoa = []\n",
    "    for group_number in range(max(atog)+1):\n",
    "        gtoa.append( [index for index, value in enumerate(atog) if value == group_number] )\n",
    "    return gtoa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execution - Group RNN **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train group RNN on classifying the object group\n",
    "\n",
    "#Load group dico and labels\n",
    "location = directory + 'labels/grouping_1/object_group_labels.txt'\n",
    "\n",
    "(labels, pred_labels, true_labels) = doMyRNN(location, family, -1) #-1 as this is a main group net (not subnet)\n",
    "\n",
    "#Save on disk to not have to retrain to get results\n",
    "location = directory + 'results/group_RNN/' + family + '/'\n",
    "np.save(location + 'labels.npy', labels)    # .npy extension is added if not given\n",
    "np.save(location + 'pred_labels.npy', pred_labels) \n",
    "np.save(location + 'true_labels.npy', true_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyze Object group RNN results\n",
    "\n",
    "#Load variables to analyze\n",
    "location = directory + 'results/group_RNN/' + family + '/'\n",
    "(labels, pred_labels, true_labels) = ( np.load(location + 'labels.npy'), \n",
    "                                       np.load(location + 'pred_labels.npy'),\n",
    "                                       np.load(location + 'true_labels.npy') )\n",
    "location = directory + 'labels/grouping_1/object_group_labels.txt'\n",
    "(_, g_labels) = action_to_group(location, family)\n",
    "\n",
    "# Run analysis: score, accuracy, conf_mat\n",
    "ite = 7\n",
    "confidence_probability_g (labels[ite,:], pred_labels[ite], true_labels[ite], g_labels)\n",
    "correct_pred = np.equal(true_labels, pred_labels) #output 0 & 1 vector\n",
    "acc = np.mean(correct_pred) #mean of above\n",
    "print('Group %s Accuracy %.2f %% with %i groups' % (family, float(100*acc), len(g_labels)))\n",
    "confusion_mtx(true_labels, pred_labels, g_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execution - Subnets RNN **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in MODEL\n",
      "in LAUNCH\n",
      "in OPTIMIZE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved subnet at epoch 0 at /home/nathan/Documents/FYP_code/LSTM1_guillermo/saved_sessions/subnets/Object/subnet_3\n",
      "Epoch  0 accuracy 53.8% in 97.8 seconds\n",
      "max_acc 53.8% at epoch  0 \n",
      "\n",
      "Epoch  1 accuracy 48.7% in 0.7 seconds\n",
      "max_acc 53.8% at epoch  0 \n",
      "\n",
      "Epoch  2 accuracy 53.8% in 0.6 seconds\n",
      "max_acc 53.8% at epoch  0 \n",
      "\n",
      "Saved subnet at epoch 3 at /home/nathan/Documents/FYP_code/LSTM1_guillermo/saved_sessions/subnets/Object/subnet_3\n",
      "Epoch  3 accuracy 66.7% in 8.7 seconds\n",
      "max_acc 66.7% at epoch  3 \n",
      "\n",
      "Epoch  4 accuracy 66.7% in 0.7 seconds\n",
      "max_acc 66.7% at epoch  3 \n",
      "\n",
      "Saved subnet at epoch 5 at /home/nathan/Documents/FYP_code/LSTM1_guillermo/saved_sessions/subnets/Object/subnet_3\n",
      "Epoch  5 accuracy 79.5% in 8.3 seconds\n",
      "max_acc 79.5% at epoch  5 \n",
      "\n",
      "Epoch  6 accuracy 76.9% in 0.8 seconds\n",
      "max_acc 79.5% at epoch  5 \n",
      "\n",
      "Epoch  7 accuracy 74.4% in 0.9 seconds\n",
      "max_acc 79.5% at epoch  5 \n",
      "\n",
      "Epoch  8 accuracy 79.5% in 0.8 seconds\n",
      "max_acc 79.5% at epoch  5 \n",
      "\n",
      "Epoch  9 accuracy 76.9% in 0.7 seconds\n",
      "max_acc 79.5% at epoch  5 \n",
      "\n",
      "Saved subnet at epoch 10 at /home/nathan/Documents/FYP_code/LSTM1_guillermo/saved_sessions/subnets/Object/subnet_3\n",
      "Epoch 10 accuracy 84.6% in 7.6 seconds\n",
      "max_acc 84.6% at epoch 10 \n",
      "\n",
      "Epoch 11 accuracy 82.1% in 0.7 seconds\n",
      "max_acc 84.6% at epoch 10 \n",
      "\n",
      "Epoch 12 accuracy 79.5% in 0.7 seconds\n",
      "max_acc 84.6% at epoch 10 \n",
      "\n",
      "Epoch 13 accuracy 82.1% in 0.6 seconds\n",
      "max_acc 84.6% at epoch 10 \n",
      "\n",
      "Saved subnet at epoch 14 at /home/nathan/Documents/FYP_code/LSTM1_guillermo/saved_sessions/subnets/Object/subnet_3\n",
      "Epoch 14 accuracy 87.2% in 7.8 seconds\n",
      "max_acc 87.2% at epoch 14 \n",
      "\n",
      "Epoch 15 accuracy 64.1% in 0.7 seconds\n",
      "max_acc 87.2% at epoch 14 \n",
      "\n",
      "Epoch 16 accuracy 51.3% in 0.7 seconds\n",
      "max_acc 87.2% at epoch 14 \n",
      "\n",
      "Epoch 17 accuracy 74.4% in 0.7 seconds\n",
      "max_acc 87.2% at epoch 14 \n",
      "\n",
      "Epoch 18 accuracy 69.2% in 0.7 seconds\n",
      "max_acc 87.2% at epoch 14 \n",
      "\n",
      "Epoch 19 accuracy 69.2% in 0.7 seconds\n",
      "max_acc 87.2% at epoch 14 \n",
      "\n",
      "Epoch 20 accuracy 74.4% in 0.7 seconds\n",
      "max_acc 87.2% at epoch 14 \n",
      "\n",
      "Saved subnet at epoch 21 at /home/nathan/Documents/FYP_code/LSTM1_guillermo/saved_sessions/subnets/Object/subnet_3\n",
      "Epoch 21 accuracy 89.7% in 7.7 seconds\n",
      "max_acc 89.7% at epoch 21 \n",
      "\n",
      "Epoch 22 accuracy 89.7% in 0.6 seconds\n",
      "max_acc 89.7% at epoch 21 \n",
      "\n",
      "Epoch 23 accuracy 82.1% in 0.6 seconds\n",
      "max_acc 89.7% at epoch 21 \n",
      "\n",
      "Epoch 24 accuracy 84.6% in 0.7 seconds\n",
      "max_acc 89.7% at epoch 21 \n",
      "\n",
      "Epoch 25 accuracy 79.5% in 0.6 seconds\n",
      "max_acc 89.7% at epoch 21 \n",
      "\n",
      "Saved subnet at epoch 26 at /home/nathan/Documents/FYP_code/LSTM1_guillermo/saved_sessions/subnets/Object/subnet_3\n",
      "Epoch 26 accuracy 92.3% in 7.7 seconds\n",
      "max_acc 92.3% at epoch 26 \n",
      "\n",
      "Epoch 27 accuracy 89.7% in 0.7 seconds\n",
      "max_acc 92.3% at epoch 26 \n",
      "\n",
      "Epoch 28 accuracy 84.6% in 0.7 seconds\n",
      "max_acc 92.3% at epoch 26 \n",
      "\n",
      "Epoch 29 accuracy 89.7% in 0.7 seconds\n",
      "max_acc 92.3% at epoch 26 \n",
      "\n",
      "Epoch 30 accuracy 84.6% in 0.7 seconds\n",
      "max_acc 92.3% at epoch 26 \n",
      "\n",
      "Epoch 31 accuracy 79.5% in 0.7 seconds\n",
      "max_acc 92.3% at epoch 26 \n",
      "\n",
      "Epoch 32 accuracy 84.6% in 0.7 seconds\n",
      "max_acc 92.3% at epoch 26 \n",
      "\n",
      "Epoch 33 accuracy 89.7% in 0.7 seconds\n",
      "max_acc 92.3% at epoch 26 \n",
      "\n",
      "Epoch 34 accuracy 89.7% in 0.7 seconds\n",
      "max_acc 92.3% at epoch 26 \n",
      "\n",
      "Epoch 35 accuracy 84.6% in 0.7 seconds\n",
      "max_acc 92.3% at epoch 26 \n",
      "\n",
      "Epoch 36 accuracy 76.9% in 0.6 seconds\n",
      "max_acc 92.3% at epoch 26 \n",
      "\n",
      "Epoch 37 accuracy 76.9% in 0.7 seconds\n",
      "max_acc 92.3% at epoch 26 \n",
      "\n",
      "Epoch 38 accuracy 92.3% in 0.7 seconds\n",
      "max_acc 92.3% at epoch 26 \n",
      "\n",
      "Epoch 39 accuracy 84.6% in 0.6 seconds\n",
      "max_acc 92.3% at epoch 26 \n",
      "\n",
      "Epoch 40 accuracy 71.8% in 0.6 seconds\n",
      "max_acc 92.3% at epoch 26 \n",
      "\n",
      "Epoch 41 accuracy 82.1% in 0.7 seconds\n",
      "max_acc 92.3% at epoch 26 \n",
      "\n",
      "Epoch 42 accuracy 87.2% in 0.7 seconds\n",
      "max_acc 92.3% at epoch 26 \n",
      "\n",
      "Epoch 43 accuracy 89.7% in 0.7 seconds\n",
      "max_acc 92.3% at epoch 26 \n",
      "\n",
      "Epoch 44 accuracy 89.7% in 0.7 seconds\n",
      "max_acc 92.3% at epoch 26 \n",
      "\n",
      "Epoch 45 accuracy 89.7% in 0.7 seconds\n",
      "max_acc 92.3% at epoch 26 \n",
      "\n",
      "Epoch 46 accuracy 92.3% in 0.6 seconds\n",
      "max_acc 92.3% at epoch 26 \n",
      "\n",
      "Saved subnet at epoch 47 at /home/nathan/Documents/FYP_code/LSTM1_guillermo/saved_sessions/subnets/Object/subnet_3\n",
      "Epoch 47 accuracy 94.9% in 7.7 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 48 accuracy 94.9% in 0.7 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 49 accuracy 89.7% in 0.7 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 50 accuracy 82.1% in 0.6 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 51 accuracy 89.7% in 0.7 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 52 accuracy 92.3% in 0.9 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 53 accuracy 84.6% in 0.9 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 54 accuracy 66.7% in 1.0 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 55 accuracy 84.6% in 0.9 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 56 accuracy 84.6% in 0.8 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 57 accuracy 82.1% in 0.7 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 58 accuracy 92.3% in 0.6 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 59 accuracy 87.2% in 0.7 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 60 accuracy 82.1% in 0.6 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 61 accuracy 82.1% in 0.7 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 62 accuracy 82.1% in 0.7 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 63 accuracy 89.7% in 0.7 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 64 accuracy 87.2% in 0.7 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 65 accuracy 89.7% in 0.7 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 66 accuracy 79.5% in 0.8 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 67 accuracy 79.5% in 0.7 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 68 accuracy 82.1% in 0.7 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 69 accuracy 82.1% in 0.8 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 70 accuracy 82.1% in 0.7 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 71 accuracy 89.7% in 0.7 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 72 accuracy 87.2% in 0.7 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 73 accuracy 87.2% in 0.8 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 74 accuracy 89.7% in 0.8 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 75 accuracy 89.7% in 0.8 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 76 accuracy 92.3% in 0.9 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 77 accuracy 84.6% in 0.8 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 78 accuracy 82.1% in 0.7 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 79 accuracy 89.7% in 0.7 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 80 accuracy 89.7% in 0.7 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 81 accuracy 87.2% in 0.7 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 82 accuracy 89.7% in 0.7 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 83 accuracy 87.2% in 0.7 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 84 accuracy 84.6% in 0.7 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 85 accuracy 89.7% in 0.7 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 86 accuracy 89.7% in 0.7 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 87 accuracy 89.7% in 0.7 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 88 accuracy 84.6% in 0.6 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 89 accuracy 74.4% in 0.7 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 90 accuracy 82.1% in 0.7 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 91 accuracy 79.5% in 0.7 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 92 accuracy 89.7% in 0.7 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 93 accuracy 89.7% in 0.6 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 94 accuracy 89.7% in 0.6 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 95 accuracy 87.2% in 0.6 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 96 accuracy 92.3% in 0.7 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 97 accuracy 84.6% in 0.6 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 98 accuracy 79.5% in 0.6 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Epoch 99 accuracy 82.1% in 0.6 seconds\n",
      "max_acc 94.9% at epoch 47 \n",
      "\n",
      "Optimization Finished!\n",
      "Group 3 \"dishsoap\" - ['close_liquid_soap', 'open_liquid_soap', 'pour_liquid_soap'] - accuracy: 94.87 %\n",
      "[0 0 1 1 2 2 0 1 1 2 2 0 0 0 1 1 2 2 0 0 1 1 2 2 2 0 1 1 2 2 0 2 1 1 1 1 1\n",
      " 2 2]\n",
      "[0 0 1 1 2 2 0 1 1 2 2 0 0 0 1 1 2 2 0 0 1 1 2 2 0 0 1 1 2 2 0 0 1 1 1 1 1\n",
      " 2 2]\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False  True  True  True  True  True  True False  True  True  True  True\n",
      "  True  True  True]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Train subnets independently of group RNN \n",
    "\n",
    "#To visualise the whole grouping of 45 actions at once as a nested list\n",
    "\"\"\"\n",
    "for i in range(26):\n",
    "    group_sub_labels = get_group_labels(i)\n",
    "    print(group_sub_labels)\n",
    "\"\"\"\n",
    "\n",
    "#Subnets are just like the orginal RNN but with a selected number of training samples from training.txt and testing.txt\n",
    "\n",
    "location = directory + 'labels/grouping_1/object_group_labels.txt'\n",
    "(atog, g_labels) = action_to_group(location, family)\n",
    "\n",
    "#do a for group_number in range(max(atog)+1):\n",
    "group_number = 3 \n",
    "num_classes = atog.count(group_number) #number of classes\n",
    "group_sub_labels = get_group_labels(group_number) #labels of classes (inside this group)\n",
    "\n",
    "if num_classes > 1: #don't train RNN with 1 output\n",
    "    \n",
    "    (labels, pred_labels, true_labels) = doMyRNN(location, family, group_number)\n",
    "\n",
    "    #Evaluating subnet\n",
    "    acc = np.mean(np.equal(true_labels, pred_labels))\n",
    "    print('Group %i \"%s\" - %s - accuracy: %.2f %%' % (group_number, g_labels[group_number], group_sub_labels, float(100*acc)))\n",
    "    print(pred_labels)\n",
    "    print(true_labels)\n",
    "    print(np.equal(true_labels, pred_labels))\n",
    "    confusion_mtx(true_labels, pred_labels, group_sub_labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Train all subnets with a for loop and save them all\n",
    "#2. Import each as pre-trained model\n",
    "#3. Gather relevant output of groupRNN \n",
    "#4. Feed it to subnet\n",
    "#5. # Get accuracy of each subnet and overall accuracy, including the outputs that did not get a subnet (1 action groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain all original data\n",
    "(test_data, test_labels, test_lengths) = create_dataset(file_testing, [], -2)\n",
    "test_data, test_labels, test_lengths = np.asarray(test_data), np.asarray(test_labels), np.asarray(test_lengths) \n",
    "\n",
    "#Link groupRNN output to subnets input using pred_labels\n",
    "location = directory + 'results/group_RNN/' + family + '/'\n",
    "pred_labels = np.load(location + 'pred_labels.npy') \n",
    "indices = [index for index, value in enumerate(pred_labels) if value == group_number]\n",
    "\n",
    "#Start for loop here\n",
    "(sub_test_data, sub_test_labels, sub_test_lengths) = test_data[indices], test_labels[indices], test_lengths[indices]\n",
    "#use sub_test_labels to compare at the end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/nathan/Documents/FYP_code/LSTM1_guillermo/saved_sessions/subnets/Object/subnet_3\n",
      "[0 1 2 2 0 1 2 2 0 0 0 1 0 0 1 0 1 2 0 0 0 1 2 2 0 1 1 1 2 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "#Load pre-trained subnet\n",
    "sess=tf.Session()    \n",
    "location = directory + 'saved_sessions/subnets/' + family + '/subnet_' + str(group_number)\n",
    "saver = tf.train.import_meta_graph(location + \".meta\")\n",
    "saver.restore(sess, location)\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "#Import variables\n",
    "x = graph.get_tensor_by_name(\"x:0\")\n",
    "y = graph.get_tensor_by_name(\"y:0\")\n",
    "seqlen = graph.get_tensor_by_name(\"seqlen:0\")\n",
    "keep_prob = graph.get_tensor_by_name(\"keep_prob:0\")\n",
    "#Maybe weights need to be loaded and put into feed_dict.. check ???\n",
    "\n",
    "#dont need to pass labels normally as they are not used\n",
    "#one_hot the label as not done in create_dataset for this part\n",
    "#sub_test_labels = np.asarray( num_to_idx(???num, ???num_classes) )\n",
    "#if not true, add 'y' to feed_dict\n",
    "\n",
    "feed_dict={x: sub_test_data, seqlen: sub_test_lengths, keep_prob: 1.0} \n",
    "prediction = graph.get_tensor_by_name(\"prediction:0\")\n",
    "#sub_pred_labels = prediction.eval(feed_dict)\n",
    "sub_pred_labels = sess.run(prediction, feed_dict) \n",
    "\n",
    "print(sub_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3 16 22 22 37 16 22 22 32  3  3 16  3  3  8 10 16 22  3 13 14 16 22 22\n",
      "  3 16 16 16 22 22 23 37]\n",
      "[3, 16, 22, 22, 3, 16, 22, 22, 3, 3, 3, 16, 3, 3, 16, 3, 16, 22, 3, 3, 3, 16, 22, 22, 3, 16, 16, 16, 22, 22, 16, 3]\n",
      "[ True  True  True  True False  True  True  True False  True  True  True\n",
      "  True  True False False  True  True  True False False  True  True  True\n",
      "  True  True  True  True  True  True False False]\n"
     ]
    }
   ],
   "source": [
    "gtoa = group_to_action(atog); list_actions = gtoa[group_number] \n",
    "original_sub_pred_labels = [list_actions[i] for i in sub_pred_labels]\n",
    "\n",
    "print(sub_test_labels)\n",
    "print(original_sub_pred_labels)\n",
    "\n",
    "correct_pred = np.equal(original_sub_pred_labels, sub_test_labels) #output 0 & 1 vector\n",
    "#append it to the rest of preds first...\n",
    "#acc = np.mean(correct_pred) #mean of above\n",
    "\n",
    "print(correct_pred)\n",
    "\n",
    "#Only care about overall accuracy of the subnet after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['charge_cell_phone']\n",
      "['clean_glasses', 'unfold_glasses']\n",
      "['close_juice_bottle', 'open_juice_bottle', 'pour_juice_bottle']\n",
      "['close_liquid_soap', 'open_liquid_soap', 'pour_liquid_soap']\n",
      "['close_milk', 'open_milk', 'pour_milk']\n",
      "['close_peanut_butter', 'open_peanut_butter']\n",
      "['drink_mug']\n",
      "['flip_pages']\n",
      "['flip_sponge', 'scratch_sponge', 'squeeze_sponge', 'wash_sponge']\n",
      "['give_card']\n",
      "['give_coin', 'receive_coin']\n",
      "['handshake', 'high_five']\n",
      "['light_candle']\n",
      "['open_letter', 'take_letter_from_enveloppe']\n",
      "['open_soda_can']\n",
      "['open_wallet']\n",
      "['pour_wine']\n",
      "['prick']\n",
      "['put_salt']\n",
      "['put_sugar', 'scoop_spoon', 'sprinkle', 'stir']\n",
      "['put_tea_bag']\n",
      "['read_letter', 'squeeze_paper', 'tear_paper']\n",
      "['toast_wine']\n",
      "['use_calculator']\n",
      "['use_flash']\n",
      "['write']\n"
     ]
    }
   ],
   "source": [
    "for i in range(26):\n",
    "    group_sub_labels = get_group_labels(i)\n",
    "    print(group_sub_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
