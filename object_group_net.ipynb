{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# based on https://danijar.com/variable-sequence-lengths-in-tensorflow/\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import functools\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import re #regexp\n",
    "\n",
    "from tensorflow.python.ops import rnn_cell, rnn\n",
    "from sklearn.metrics import confusion_matrix #compute confusion_matrix\n",
    "from matplotlib import pyplot as plt #display confusion_matrix\n",
    "\n",
    "directory = '/home/nathan/Documents/FYP_code/LSTM1_guillermo/'\n",
    "directory_dataset = directory + 'data_nathan/'\n",
    "\n",
    "file_training = \"training.txt\"\n",
    "file_testing = \"testing.txt\" \n",
    "\n",
    "feat_size = 63 #21 joints * 3 dimensions (xyz)\n",
    "\n",
    "batch_size = 20\n",
    "padding_size = 300\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.003\n",
    "n_epochs = 100\n",
    "\n",
    "# Network Parameters\n",
    "seq_max_len = padding_size # Sequence max length\n",
    "n_hidden = 100 # hidden layer num of features\n",
    "max_seq_l = 120\n",
    "\n",
    "p_dropout = 0.2 #1 #0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract action-to-group dictionary & group labels\n",
    "def action_to_group (location, family):\n",
    "\n",
    "    with open(location, 'r') as f:\n",
    "        pattern = '_(\\w+)' #for Object by default\n",
    "        if family == 'Motion': pattern = '(\\w+)_' #add patterns like this as a list\n",
    "        regexp = re.compile(pattern)\n",
    "        get = re.findall( regexp, f.read() )\n",
    "\n",
    "    atog = list() #action to group dictionary \n",
    "    g_labels = list() \n",
    "    for i in get:\n",
    "        #check if group already exists\n",
    "        if i not in g_labels: #extract group number as index of word in submitted & add it to action group dictionary\n",
    "            g_labels.append(i)\n",
    "        atog.append( g_labels.index(i) ) \n",
    "\n",
    "    #num_g = len(g_labels) FYI\n",
    "    \n",
    "    return (atog, g_labels) \n",
    "\n",
    "def read_data(filename):\n",
    "    # Reads file containing features and returns features indexed by time\n",
    "    x = []\n",
    "    tmp_length = 0\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            numbers_str = line.split()\n",
    "            nums_float = [float(a) for a in numbers_str]\n",
    "            x.append(nums_float)\n",
    "            tmp_length =tmp_length+1\n",
    "            # print(len(x))\n",
    "    # x.extend([[0.0]*feat_size]*(padding_size-len(x)+1))\n",
    "    f.close() #necessary ? supposed to be automatic\n",
    "    tmp_val = np.min([tmp_length-1,max_seq_l])\n",
    "    #print(tmp_val)\n",
    "    return x[1:]  # ignore de first line (num of frames)\n",
    "\n",
    "def read_config(filename):\n",
    "    # Reads config file and returns filenames and class label\n",
    "    x = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            line_split = line.split()\n",
    "            x.append(line_split)\n",
    "    f.close()\n",
    "    return x\n",
    "\n",
    "# one hot encoding\n",
    "def num_to_idx(num, num_classes):\n",
    "    vec = np.zeros( shape=num_classes, dtype=np.float) #hardcode here\n",
    "    vec[num] = 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Called once to load training/testing data in MODEL\n",
    "def create_dataset(filename, atog, group_number):\n",
    "    # dataset is organized as NxLxD (N = num samples, L temporal length with padding, D feature dimension\n",
    "    # labels is NxY where Y is one hot label vector\n",
    "    \n",
    "    num_classes = max(atog) + 1 # as labelled 0 to 25 but want 26 as number of classes\n",
    "\n",
    "    dataset, labels, lengths = [], [], []\n",
    "    files = read_config(filename)\n",
    "\n",
    "    for i in files:\n",
    "\n",
    "        if group_number == -1: #Build group main RNN\n",
    "            \n",
    "            tmp_data = read_data(directory_dataset + i[0]) \n",
    "            num =  atog[ int( i[1] )] #only used for one hot encoding in the line below\n",
    "            tmp_labels = np.transpose(num_to_idx(num, num_classes)) \n",
    "\n",
    "            #i[0] =  a01s01r01.txt (for eg)\n",
    "            #i[1] = number between 0 and 44 (or as many classes there is)\n",
    "            #tmp_labels = [0, 0, 1, 0, .. 0] = one-hot encoding of class value\n",
    "            #tmp_data = list with variable len around 300 & each item in the list is a nested list of len 63 (=feat_size)\n",
    "        \n",
    "        else: #Build action subnet RNN \n",
    "            \n",
    "            num_classes = atog.count(group_number) #number of times this group_number is mentioned in atog\n",
    "            num =  atog[ int( i[1] )] #gives group number of current action\n",
    "            if num ==  group_number: #select data only if belongs to group \n",
    "                tmp_data = read_data(directory_dataset + i[0]) #add to dataset if part of the group\n",
    "                tmp_labels = np.transpose(num_to_idx(num, num_classes)) #add labels as well      \n",
    "            \n",
    "        if len(tmp_data)<300: #why 300, is that the longest sequence ? 300 = padding_size btw...\n",
    "            \n",
    "            #records tmp_data initial length before padding\n",
    "            #pads tmp_data with zeros until padding_size (300) so len(tmp_data) = 300 always with len 63 items\n",
    "                \n",
    "            lengths.append(len(tmp_data))\n",
    "            tmp_data.extend([ [0.0] * feat_size ] * (padding_size - len(tmp_data)))  \n",
    "\n",
    "            dataset.append(tmp_data)\n",
    "            labels.append(tmp_labels) \n",
    "    \n",
    "    # all 0..565 lists with item as nested lists of size (300, 26, 1)\n",
    "    return dataset, labels, lengths\n",
    "\n",
    "#Called once when training RNN\n",
    "def batch_generation(data,labels,lengths):\n",
    "    \n",
    "    num_classes = np.size(labels,1) #labels is a one hot encoded numpy array. This returns number of columns (=groups).\n",
    "    \n",
    "    nsamples,_,_ = data.shape\n",
    "\n",
    "    indices = np.arange(nsamples)\n",
    "    np.random.shuffle(indices)\n",
    "    num_batches = int(np.floor(nsamples/batch_size))\n",
    "    not_exact = 0\n",
    "\n",
    "    if nsamples%batch_size != 0:\n",
    "        not_exact = 1\n",
    "\n",
    "    batches_data = np.empty(shape=[num_batches+not_exact,batch_size,padding_size,feat_size])\n",
    "    batches_labels = np.empty(shape=[num_batches+not_exact,batch_size,num_classes]) #hardcode here with num_classes\n",
    "    batches_lengths = np.empty(shape=[num_batches + not_exact, batch_size],dtype=np.int32)\n",
    "\n",
    "    for x in range(num_batches):\n",
    "        batches_data[x, :, :, :] = data[indices[batch_size*x:batch_size*(x+1)], :, :]\n",
    "        batches_labels[x,:,:] = labels[indices[batch_size*x:batch_size*(x+1)], :]\n",
    "        batches_lengths[x,:] = lengths[indices[batch_size*x:batch_size*(x+1)]]\n",
    "\n",
    "    if not_exact > 0:\n",
    "        to_complete = nsamples%batch_size\n",
    "        tmp_data = data[indices[batch_size*num_batches:batch_size*num_batches+to_complete],:,:]\n",
    "        tmp_random = np.random.randint(0,nsamples-to_complete,batch_size-to_complete) # we complete last batch with random samples\n",
    "\n",
    "        batches_data[num_batches]=np.concatenate((tmp_data,data[tmp_random,:,:]),axis=0)\n",
    "\n",
    "        tmp_labels = labels[indices[batch_size*num_batches:batch_size*num_batches+to_complete],:]\n",
    "        batches_labels[num_batches] = np.concatenate((tmp_labels,labels[indices[tmp_random],:]))\n",
    "        tmp_lengths = lengths[indices[batch_size*num_batches:batch_size*num_batches+to_complete]]\n",
    "        batches_lengths[num_batches] = np.concatenate((tmp_lengths,lengths[indices[tmp_random]]))\n",
    "\n",
    "    return batches_data, batches_labels, batches_lengths, num_batches+not_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamicRNN(x, seqlen, weights, biases, keep_prob):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Permuting batch_size and n_steps\n",
    "    x = tf.transpose(x, [1, 0, 2])\n",
    "    # Reshaping to (n_steps*batch_size, n_input)\n",
    "    x = tf.reshape(x, [-1, feat_size])\n",
    "    # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.split(x, seq_max_len, 0) # tf.split(value, num_or_size_splits, axis)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "    \n",
    "    #Operator adding dropout to inputs and outputs of the given cell.\n",
    "    lstm = rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob) \n",
    "    \n",
    "    #Added to have 2 layers LSTM, not used here.\n",
    "    layers = 1\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * layers)\n",
    "\n",
    "    # Get lstm cell output, providing 'sequence_length' will perform dynamic calculation.\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cell, x, dtype=tf.float32, sequence_length=seqlen)\n",
    "    \n",
    "    # When performing dynamic calculation, we must retrieve the last\n",
    "    # dynamically computed output, i.e, if a sequence length is 10, we need\n",
    "    # to retrieve the 10th output.\n",
    "    # However TensorFlow doesn't support advanced indexing yet, so we build\n",
    "    # a custom op that for each sample in batch size, get its length and\n",
    "    # get the corresponding relevant output.\n",
    "\n",
    "    # 'outputs' is a list of output at every timestep, we pack them in a Tensor\n",
    "    # and change back dimension to [batch_size, n_step, n_input]\n",
    "    outputs = tf.stack(outputs)\n",
    "    outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "\n",
    "    # Hack to build the indexing and retrieve the right output.\n",
    "    batch_size = tf.shape(outputs)[0]\n",
    "    # Start indices for each sample\n",
    "    index = tf.range(0, batch_size) * seq_max_len + (seqlen - 1)\n",
    "    # Indexing\n",
    "    outputs = tf.gather(tf.reshape(outputs, [-1, n_hidden]), index)\n",
    "\n",
    "    # Linear activation, using outputs computed above\n",
    "    return tf.matmul(outputs, weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doMyRNN(location, family, group_number):\n",
    "\n",
    "    # ==========\n",
    "    #   MODEL\n",
    "    # ==========\n",
    "    print('in MODEL')\n",
    "\n",
    "    tf.reset_default_graph() #Clear computational graph to prevent error\n",
    "\n",
    "    # Load training and testing data\n",
    "    (atog, _) = action_to_group(location, family)\n",
    "    train_data, train_labels, train_lengths = create_dataset(file_training, atog, group_number)\n",
    "    test_data, test_labels, test_lengths = create_dataset(file_testing, atog, group_number)\n",
    "\n",
    "    #cast to numpy array\n",
    "    train_data = np.asarray(train_data)\n",
    "    train_labels = np.asarray(train_labels)\n",
    "    train_lengths = np.asarray(train_lengths,dtype=np.int32)\n",
    "    test_data = np.asarray(test_data)\n",
    "    test_labels = np.asarray(test_labels)\n",
    "    test_lengths = np.asarray(test_lengths)\n",
    "\n",
    "    (samples, rows, row_size) = train_data.shape\n",
    "\n",
    "    n_classes = np.size(train_labels,1)\n",
    "\n",
    "    # tf Graph input\n",
    "    x = tf.placeholder(\"float\", [None, seq_max_len, feat_size])\n",
    "    y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "    # A placeholder for indicating each sequence length\n",
    "    seqlen = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "    # Define weights\n",
    "    weights = {\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    # ==========\n",
    "    #   LAUNCH\n",
    "    # ==========\n",
    "    print('in LAUNCH')\n",
    "\n",
    "    pred = dynamicRNN(x, seqlen, weights, biases, keep_prob)\n",
    "\n",
    "    # Evaluate model \n",
    "    prediction = tf.argmax(pred,1) # for each prediction, keep class with highest level of confidence (tests X classes)\n",
    "    correct_pred = tf.equal(prediction, tf.argmax(y,1)) #output 0 & 1 vector, y is supposed to have true labels\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32)) #mean of above\n",
    "\n",
    "    #Create a saver object which will save all the variables\n",
    "    #saver = tf.train.Saver()\n",
    "\n",
    "    # ============\n",
    "    #   OPTIMIZE\n",
    "    # ============\n",
    "    print('in OPTIMIZE')\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = pred, labels = y))\n",
    "    # optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    # optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Launch the graph\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        sess.run(init)\n",
    "\n",
    "        max_acc = 0.0;\n",
    "        max_epoch = 0;\n",
    "        best_labels = []\n",
    "\n",
    "        # Keep training until reach max iterations\n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "            (batch_x, batch_y, batch_seqlen, n_batches) = batch_generation(train_data, train_labels, train_lengths)\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            for i in range(n_batches):\n",
    "\n",
    "\n",
    "                # Run optimization op (backprop)\n",
    "                sess.run(optimizer, feed_dict={x: batch_x[i,:,:,:], y: batch_y[i,:,:],\n",
    "                                                        seqlen: batch_seqlen[i,:], keep_prob: p_dropout})\n",
    "                # Calculate batch accuracy\n",
    "                # acc = sess.run(accuracy, feed_dict={x: batch_x[i,:,:,:], y: batch_y[i,:,:],\n",
    "                #                                     seqlen: batch_seqlen[i]})\n",
    "                # # Calculate batch loss\n",
    "                # loss = sess.run(cost, feed_dict={x: batch_x[i,:,:,:], y: batch_y[i,:,:],\n",
    "                #                                     seqlen: batch_seqlen[i]})\n",
    "\n",
    "            # Test accuracy on this epoch    \n",
    "            test_acc = sess.run(accuracy, feed_dict={x: test_data, y: test_labels, seqlen: test_lengths, keep_prob: 1.0})\n",
    "\n",
    "            #Save best testing results\n",
    "            if test_acc > max_acc: \n",
    "\n",
    "                #Save the best accuracy and predictions\n",
    "                max_acc = test_acc; max_epoch = epoch\n",
    "                #gives the confidence score of every class at output along columns (dim 1)\n",
    "                labels = pred.eval(feed_dict={x: test_data, y: test_labels, seqlen: test_lengths, keep_prob: 1.0})\n",
    "                # pred_labels = tf.argmax(labels, 1) OR\n",
    "                pred_labels = prediction.eval(feed_dict={x: test_data, y: test_labels, seqlen: test_lengths, keep_prob: 1.0})\n",
    "                #test_labels is one_hot. Want 1-D like best_labels\n",
    "                true_labels = np.asarray( [ np.where(r==1)[0][0] for r in test_labels ], dtype=np.int64)\n",
    "\n",
    "                #Save the graph (the best model)\n",
    "                #saver_path = saver.save(sess, ... ) \n",
    "                #print(\"Saving model at epoch %i and path %s\" % (epoch, saver_path))\n",
    "\n",
    "            print('Epoch {:2d} accuracy {:3.1f}% in {:3.1f} seconds'.format(epoch, 100 * test_acc, time.time() - start))\n",
    "            print('max_acc {:3.1f}% at epoch {:2d} \\n'.format(max_acc*100, max_epoch))\n",
    "\n",
    "        print (\"Optimization Finished!\")\n",
    "\n",
    "        return (labels, pred_labels, true_labels)\n",
    "\n",
    "        # ToDo\n",
    "        # Get training accuracy and plot it along test accuracy \n",
    "        # Add a validation set accuracy to train without overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display confusion matrix\n",
    "def confusion_mtx(true, pred, name_labels):\n",
    "    \n",
    "    num_classes = len(name_labels)\n",
    "    cm = confusion_matrix(true, pred)\n",
    "\n",
    "    fig = plt.figure(figsize=(10,10), dpi=100)\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(cm)\n",
    "    fig.colorbar(cax)\n",
    "    plt.xticks(range(num_classes), name_labels, rotation=90)\n",
    "    plt.yticks(range(num_classes), name_labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "    \n",
    "    #TODO\n",
    "    #Adjust the matrix scale (0 to 45 to the relevant scaling 0 to 100%)\n",
    "    #Have a checkerboard figure to track classes easily\n",
    "    #Have white background for better error reading for colorbar\n",
    "    #Rotate 45 the vertical axis\n",
    "\n",
    "#DO NOT USE HERE BEFORE UPDATING AS ATOG ON A GROUP WILL FUCK IT UP UNLESS YOU PASS THE ORIGINIAL (out of45) class values\n",
    "#Display confidence level bar histogram \n",
    "def confidence_probability_g (scores, pred_class, true_class, g_labels):\n",
    "    \n",
    "    format_scores = scores - np.amin(scores)\n",
    "    format_scores = format_scores / np.amax(format_scores)\n",
    "    \n",
    "    #y = scores\n",
    "    y = format_scores #probabilities made from normalising on this 1 score (not on the 569 scores)\n",
    "    x = range(len(scores)) #as much scores as there is in y (45)\n",
    "    \n",
    "    plt.figure(figsize=(20,4))\n",
    "    \n",
    "    barlist = plt.bar(x,y, width=0.7)\n",
    "\n",
    "    import randomcolor\n",
    "    colors = randomcolor.RandomColor().generate(count=len(g_labels)) #as many group or actions    \n",
    "    for i in range(len(barlist)): barlist[i].set_color(colors[i])\n",
    "    \n",
    "    plt.title('Predicted class ' + str(pred_class) + ' ' + g_labels[pred_class] \n",
    "                 + ' for class ' + str(true_class) + ' ' + g_labels[true_class],\n",
    "                 fontsize=18)\n",
    "    \n",
    "    plt.xticks(x, g_labels, rotation=90, fontsize=18)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    #ToDo\n",
    "    # Be able to know the name of sample seq that fails "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load group dico and labels\n",
    "family = 'Object' \n",
    "location = directory + 'labels/grouping_1/object_group_labels.txt'\n",
    "\n",
    "#Train RNN on classifying the object group\n",
    "(labels, pred_labels, true_labels) = doMyRNN(location, family, -1) #-1 as this is a main group net (not subnet)\n",
    "\n",
    "#Save on disk to not have to retrain to get results\n",
    "location = directory + 'results/group_RNN/' + family + '/'\n",
    "np.save(location + 'labels.npy', labels)    # .npy extension is added if not given\n",
    "np.save(location + 'pred_labels.npy', pred_labels) \n",
    "np.save(location + 'true_labels.npy', true_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Object group RNN results\n",
    "\n",
    "#Load variables to analyze\n",
    "family = 'Object'\n",
    "location = directory + 'results/group_RNN/' + family + '/'\n",
    "(labels, pred_labels, true_labels) = ( np.load(location + 'labels.npy'), \n",
    "                                       np.load(location + 'pred_labels.npy'),\n",
    "                                       np.load(location + 'true_labels.npy') )\n",
    "location = directory + 'labels/grouping_1/object_group_labels.txt'\n",
    "(_, g_labels) = action_to_group(location, family)\n",
    "\n",
    "# Run analysis: score, accuracy, conf_mat\n",
    "ite = 0\n",
    "confidence_probability_g (labels[ite,:], pred_labels[ite], true_labels[ite], g_labels)\n",
    "correct_pred = np.equal(true_labels, pred_labels) #output 0 & 1 vector\n",
    "acc = np.mean(correct_pred) #mean of above\n",
    "print('Group %s Accuracy %.2f %% with %i groups' % (family, float(100*acc), len(g_labels)))\n",
    "confusion_mtx(true_labels, pred_labels, g_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_labels(group_number):\n",
    "    with open(directory + 'labels/name_of_labels_original.txt') as f:\n",
    "        all_45_action_labels = [word for line in f for word in line.split()]\n",
    "    action_labels_inside_this_group = [all_45_action_labels[i] for i in range(len(atog)) if atog[i] == group_number]\n",
    "    return action_labels_inside_this_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train sub nets independently of group RNN\n",
    "family = 'Object'\n",
    "location = directory + 'labels/grouping_1/object_group_labels.txt'\n",
    "(atog, g_labels) = action_to_group(location, family)\n",
    "\n",
    "group_number = 0\n",
    "\n",
    "print('This is group %i %s' % group_number, g_labels[group_number])\n",
    "\n",
    "(labels, pred_labels, true_labels) = doMyRNN(location, family, group_number)\n",
    "\n",
    "print(true_labels.shape)\n",
    "print(pred_labels.shape)\n",
    "#Test the above function with group_number 0...\n",
    "#Do not need to create subnets for groups with 1 action in it!\n",
    "\n",
    "\n",
    "\n",
    "#To visualise the family grouping at once:\n",
    "#for i in range(26):\n",
    "#    group_sub_labels = get_group_labels(i)\n",
    "#    print(group_sub_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select relevant outputs of group RNN and feed to the right subnet.\n",
    "#Train subnets before as it will depend on it, no need for group RNN to train a subnet, just need to select\n",
    "# PSEUDOCODE\n",
    "# get atog -> feed labels to each subnet according to atog (use domyRNN while saving session and graph ?? or just session)\n",
    "# get training/testing acc of subnet and save it for a future restoration \n",
    "# gather in relevant pack the output of groupRNN to feed to subnet (a for loop that does subnet by subnet)\n",
    "# do a sess.run on subnet with output of groupRNN\n",
    "# Get accuracy of each subnet and overall subnet accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thinking about the subnets\n",
    "\n",
    "\n",
    "# What parts of the above code can you save and load for the subnets\n",
    "# check on graph to load problems -> although RNN trained differently with different outputs, could keep a part in \n",
    "# common right, which one is it ?\n",
    "# check dropout is really happening, if not rerun 1st experiment as well\n",
    "# add patience training of 20\n",
    "\n",
    "# Need to train them on the right group sample (26 subnets)\n",
    "# Need to feed them the result of group RNN (diff than above)\n",
    "# Then you need to trace back what the sample was to check if its real or not. Really ?\n",
    "# Then combine all outputs in one\n",
    "\n",
    "#Subnets are just like the orginal RNN but with a selected number of training samples from training.txt and testing.txt\n",
    "#Need to gather output of group RNN to feed to subnets\n",
    "#Once it is trained, it needs to be used again with new labels 'sess.run' to evaluaet results on those data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['charge_cell_phone']\n",
      "['clean_glasses', 'unfold_glasses']\n",
      "['close_juice_bottle', 'open_juice_bottle', 'pour_juice_bottle']\n",
      "['close_liquid_soap', 'open_liquid_soap', 'pour_liquid_soap']\n",
      "['close_milk', 'open_milk', 'pour_milk']\n",
      "['close_peanut_butter', 'open_peanut_butter']\n",
      "['drink_mug']\n",
      "['flip_pages']\n",
      "['flip_sponge', 'scratch_sponge', 'squeeze_sponge', 'wash_sponge']\n",
      "['give_card']\n",
      "['give_coin', 'receive_coin']\n",
      "['handshake', 'high_five']\n",
      "['light_candle']\n",
      "['open_letter', 'take_letter_from_enveloppe']\n",
      "['open_soda_can']\n",
      "['open_wallet']\n",
      "['pour_wine']\n",
      "['prick']\n",
      "['put_salt']\n",
      "['put_sugar', 'scoop_spoon', 'sprinkle', 'stir']\n",
      "['put_tea_bag']\n",
      "['read_letter', 'squeeze_paper', 'tear_paper']\n",
      "['toast_wine']\n",
      "['use_calculator']\n",
      "['use_flash']\n",
      "['write']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
