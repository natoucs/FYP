{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from myfunctions import action_to_group, get_group_labels, read_data, read_config, num_to_idx \n",
    "directory_dataset = '/home/nathan/Documents/FYP_code/LSTM1_guillermo/data_nathan/'\n",
    "\n",
    "feat_size = 63 #21 joints * 3 dimensions (xyz)\n",
    "batch_size = 20\n",
    "padding_size = 300\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.003\n",
    "n_epochs = 200\n",
    "\n",
    "# Network Parameters\n",
    "seq_max_len = padding_size # Sequence max length\n",
    "n_hidden = 100 # hidden layer num of features\n",
    "p_dropout = 0.5\n",
    "\n",
    "#Called to load training/testing data \n",
    "def create_dataset(filename, group_number):\n",
    "    # dataset is organized as NxLxD (N = num samples, L temporal length with padding, D feature dimension\n",
    "    # labels is NxY where Y is one hot label vector\n",
    "    \n",
    "    # -3 sends back labels as actions number (0..45) with one hot enconding (000..1..00)\n",
    "    # -2 sends back labels as actions number (0..45) with no one hot enconding (1 or 2 or 45)\n",
    "    # -1 sends back labels as groups number (0..12or26) with one hot encoding (000..1..00)\n",
    "    # 0> sends back labels of only this group with one hot enconding\n",
    "\n",
    "    dataset, labels, lengths = [], [], []\n",
    "    files = read_config(filename)\n",
    "    \n",
    "    if group_number == -3: #standard 45 actions RNN\n",
    "        num_classes = 45\n",
    "    elif group_number == -2: #groupRNN to subnets (testing phase)\n",
    "        #num_classes = 45 #not needed in the function when -2 activated\n",
    "        pass\n",
    "    elif group_number == -1: #groupRNN\n",
    "        num_classes = max(atog) + 1 \n",
    "        #pass\n",
    "    else: #subnets\n",
    "        list_actions = gtoa[group_number] #gives list of actions in current group number\n",
    "        num_classes = len(list_actions)\n",
    "        \n",
    "    for i in files:\n",
    "        \n",
    "        if group_number == -3:  #standard 45 actions RNN\n",
    "            \n",
    "            tmp_data = read_data(directory_dataset + i[0]) #i[0] =  a01s01r01.txt (for eg)\n",
    "            num =  int( i[1] )\n",
    "            tmp_labels = np.transpose(num_to_idx(num, num_classes)) #one hot encoding\n",
    "        \n",
    "        elif group_number == -2: #Feed output of groupRNN to pre-trained subnet (testing phase)\n",
    "            \n",
    "            tmp_data = read_data(directory_dataset + i[0]) #i[0] =  a01s01r01.txt (for eg)\n",
    "            tmp_labels = int(i[1]) #will one hot later as first need to dispatch data to relevant subnet using labels\n",
    "        \n",
    "        elif group_number == -1: #Build groupRNN\n",
    "            \n",
    "            tmp_data = read_data(directory_dataset + i[0]) \n",
    "            num =  atog[ int( i[1] )] #only used for one hot encoding in the line below\n",
    "            tmp_labels = np.transpose(num_to_idx(num, num_classes)) \n",
    "            #i[0] =  a01s01r01.txt (for eg)\n",
    "            #i[1] = number between 0 and 44 (or as many classes there is)\n",
    "            #tmp_labels = [0, 0, 1, 0, .. 0] = one-hot encoding of class value\n",
    "            #tmp_data = list with variable len around 300 & each item in the list is a nested list of len 63 (=feat_size)\n",
    "        \n",
    "        else: #Build subnet RNN\n",
    "            \n",
    "            num = int( i[1] ) #original action number\n",
    "            if num in list_actions: #select data only if belongs to group \n",
    "                tmp_data = read_data(directory_dataset + i[0]) #add to dataset if part of the group\n",
    "                tmp_labels = np.transpose(num_to_idx(list_actions.index(num), num_classes)) #add labels as well     \n",
    "            else: continue\n",
    "            \n",
    "        if len(tmp_data)<300: #why 300, is that the longest sequence ? 300 = padding_size btw...\n",
    "            \n",
    "            #records tmp_data initial length before padding\n",
    "            #pads tmp_data with zeros until padding_size (300) so len(tmp_data) = 300 always with len 63 items\n",
    "                \n",
    "            lengths.append(len(tmp_data))\n",
    "            tmp_data.extend([ [0.0] * feat_size ] * (padding_size - len(tmp_data)))  \n",
    "\n",
    "            dataset.append(tmp_data)\n",
    "            labels.append(tmp_labels) \n",
    "        \n",
    "    # all 0..565 lists with item as nested lists of size (300, 26, 1)\n",
    "    \n",
    "    return np.asarray(dataset), np.asarray(labels), np.asarray(lengths,dtype=np.int32)\n",
    "\n",
    "#Called once when training RNN\n",
    "def batch_generation(data,labels,lengths):\n",
    "    \n",
    "    num_classes = np.size(labels,1) #labels is a one hot encoded numpy array. This returns number of columns (=groups).\n",
    "    \n",
    "    nsamples,_,_ = data.shape\n",
    "\n",
    "    indices = np.arange(nsamples) #np.arange(3) -> array([0, 1, 2])\n",
    "    np.random.shuffle(indices) #shuffle the indices\n",
    "    num_batches = int(np.floor(nsamples/batch_size)) #round to inferior number so = 0 if batch_size bigger than nsamples\n",
    "    not_exact = 0\n",
    "\n",
    "    if nsamples%batch_size != 0: #happens all the time unless nsamples is lucky multiple of batch_size\n",
    "        not_exact = 1\n",
    "    \n",
    "    #declare empty arrays to contain the batches, dimensions are right\n",
    "    batches_data = np.empty(shape=[num_batches+not_exact,batch_size,padding_size,feat_size])\n",
    "    batches_labels = np.empty(shape=[num_batches+not_exact,batch_size,num_classes]) \n",
    "    batches_lengths = np.empty(shape=[num_batches + not_exact, batch_size],dtype=np.int32)\n",
    "\n",
    "    for x in range(num_batches):\n",
    "        batches_data[x, :, :, :] = data[indices[batch_size*x:batch_size*(x+1)], :, :]\n",
    "        batches_labels[x,:,:] = labels[indices[batch_size*x:batch_size*(x+1)], :]\n",
    "        batches_lengths[x,:] = lengths[indices[batch_size*x:batch_size*(x+1)]]\n",
    "\n",
    "    if not_exact > 0:\n",
    "        \n",
    "        to_complete = nsamples%batch_size\n",
    "        \n",
    "        #nsamples is too small, reuse the samples from previous batch, taken randomly to complete this batch\n",
    "        tmp_random = np.random.randint(0,nsamples,batch_size-to_complete) # we complete last batch with random samples\n",
    "        #prints list of indices it will take randomly\n",
    "        \n",
    "        #[num_batches] refers to the last batch that is not complete\n",
    "        tmp_data = data[indices[batch_size*num_batches:batch_size*num_batches+to_complete],:,:]\n",
    "        batches_data[num_batches]=np.concatenate((tmp_data,data[tmp_random,:,:]),axis=0)\n",
    "\n",
    "        tmp_labels = labels[indices[batch_size*num_batches:batch_size*num_batches+to_complete],:]\n",
    "        batches_labels[num_batches] = np.concatenate((tmp_labels,labels[indices[tmp_random],:]))\n",
    "        \n",
    "        tmp_lengths = lengths[indices[batch_size*num_batches:batch_size*num_batches+to_complete]]\n",
    "        batches_lengths[num_batches] = np.concatenate((tmp_lengths,lengths[indices[tmp_random]]))\n",
    "\n",
    "    return batches_data, batches_labels, batches_lengths, num_batches+not_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full example for my blog post at:\n",
    "# https://danijar.com/building-variational-auto-encoders-in-tensorflow/\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tfd = tf.contrib.distributions\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def make_encoder(data, code_size):\n",
    "  x = tf.layers.flatten(data)\n",
    "  x = tf.layers.dense(x, 200, tf.nn.relu)\n",
    "  x = tf.layers.dense(x, 200, tf.nn.relu)\n",
    "  loc = tf.layers.dense(x, code_size)\n",
    "  scale = tf.layers.dense(x, code_size, tf.nn.softplus)\n",
    "  return tfd.MultivariateNormalDiag(loc, scale)\n",
    "\n",
    "\n",
    "def make_prior(code_size):\n",
    "  loc = tf.zeros(code_size)\n",
    "  scale = tf.ones(code_size)\n",
    "  return tfd.MultivariateNormalDiag(loc, scale)\n",
    "\n",
    "\n",
    "def make_decoder(code, data_shape):\n",
    "  x = code\n",
    "  x = tf.layers.dense(x, 200, tf.nn.relu)\n",
    "  x = tf.layers.dense(x, 200, tf.nn.relu)\n",
    "  logit = tf.layers.dense(x, np.prod(data_shape))\n",
    "  logit = tf.reshape(logit, [-1] + data_shape)\n",
    "\n",
    "  #return tfd.Independent(tfd.Bernoulli(logit), 2)\n",
    "  return  tfd.Normal(loc=0., scale=1.)\n",
    "\n",
    "\n",
    "def plot_codes(ax, codes, labels):\n",
    "  ax.scatter(codes[:, 0], codes[:, 1], s=2, c=labels, alpha=0.1)\n",
    "  ax.set_aspect('equal')\n",
    "  ax.set_xlim(codes.min() - .1, codes.max() + .1)\n",
    "  ax.set_ylim(codes.min() - .1, codes.max() + .1)\n",
    "  ax.tick_params(\n",
    "      axis='both', which='both', left=False, bottom=False,\n",
    "      labelleft=False , labelbottom=False)\n",
    "\n",
    "\n",
    "def plot_samples(ax, samples):\n",
    "  for index, sample in enumerate(samples):\n",
    "    ax[index].imshow(sample, cmap='gray')\n",
    "    ax[index].axis(False)\n",
    "\n",
    "#data = tf.placeholder(tf.float32, [None, 28, 28])\n",
    "data = tf.placeholder(tf.float32, [None, 300, 63])\n",
    "\n",
    "make_encoder = tf.make_template('encoder', make_encoder)\n",
    "make_decoder = tf.make_template('decoder', make_decoder)\n",
    "\n",
    "prior = make_prior(code_size=2)\n",
    "posterior = make_encoder(data, code_size=2)\n",
    "code = posterior.sample()\n",
    "\n",
    "#likelihood = make_decoder(code, [28, 28]).log_prob(data)\n",
    "likelihood = make_decoder(code, [300, 63]).log_prob(data)\n",
    "\n",
    "divergence = tfd.kl_divergence(posterior, prior)\n",
    "elbo = tf.reduce_mean(likelihood - divergence)\n",
    "optimize = tf.train.AdamOptimizer(0.001).minimize(-elbo)\n",
    "\n",
    "#samples = make_decoder(prior.sample(10), [28, 28]).mean()\n",
    "samples = make_decoder(prior.sample(10), [300, 63]).mean()\n",
    "\n",
    "#mnist = input_data.read_data_sets('MNIST_data/')\n",
    "train_data, train_labels, train_lengths = create_dataset('training.txt', -3)\n",
    "test_data, test_labels, test_lengths = create_dataset('training.txt', -3)\n",
    "train_data = np.asarray(train_data)\n",
    "train_labels = np.asarray(train_labels)\n",
    "train_lengths = np.asarray(train_lengths)\n",
    "test_data = np.asarray(test_data)\n",
    "test_labels = np.asarray(test_labels)\n",
    "test_lengths = np.asarray(test_lengths)\n",
    "(batch_x, batch_y, batch_seqlen, n_batches) = batch_generation(train_data, train_labels, train_lengths)\n",
    "\n",
    "##fig, ax = plt.subplots(nrows=20, ncols=11, figsize=(10, 20))\n",
    "\n",
    "with tf.train.MonitoredSession() as sess:\n",
    "  for epoch in range(10):\n",
    "    \n",
    "    #feed = {data: mnist.test.images.reshape([-1, 28, 28])}\n",
    "    feed = {data: test_data}\n",
    "    \n",
    "    test_elbo, test_codes = sess.run([elbo, code], feed)\n",
    "    \n",
    "    print('Epoch', epoch, 'test_elbo', test_elbo, 'test_codes[0]', test_codes[0])\n",
    "    \n",
    "    ##ax[epoch, 0].set_ylabel('Epoch {}'.format(epoch))\n",
    "    ##plot_codes(ax[epoch, 0], test_codes, mnist.test.labels) #plot codes where ?\n",
    "    ##plot_codes(ax[epoch, 0], test_codes, train_labels)\n",
    "    ##plot_samples(ax[epoch, 1:], test_samples) #not working\n",
    "    \n",
    "    \n",
    "    for i in range(30):\n",
    "    #for _ in range(600):\n",
    "      #feed = {data: mnist.train.next_batch(100)[0].reshape([-1, 28, 28])}\n",
    "      feed = {data: batch_x[i] } #batch_x is (30, 20, 300, 63)\n",
    "    \n",
    "      sess.run(optimize, feed)\n",
    "        \n",
    "##plt.savefig('vae-mnist.png', dpi=300, transparent=True, bbox_inches='tight')\n",
    "##plt.savefig('vae-actist.png', dpi=300, transparent=True, bbox_inches='tight') #algo not working so not point saving atm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_codes.shape (10000, 2) #why returns 10000 ?\n",
    "#test_samples.shape (10, 28, 28)\n",
    "#test_elbo.shape ()\n",
    "#mnist.test.images.reshape([-1, 28, 28]) (10000, 28, 28)\n",
    "#mnist.test.labels (10000,)\n",
    "#mnist.train.next_batch(100)[0].shape (100, 784) \n",
    "## 10000 images each of dimensions 28*28, 10 samples of those images per epoch, 2 dimensions for the embedding code.\n",
    "## MNIST:The training set contains 60000 examples, and the test set 10000 examples.\n",
    "# Pass input data as numpy array of dimension (number of samples, dimA, dimB) -> (595, 300, 63)\n",
    "# No labels to do VAE (unsupervised learning).\n",
    "# For distrib: Normal (0,1)\n",
    "# For visualisation ?\n",
    "# Minimize the elbo ?\n",
    "# Set code_size ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** ORIGINAL VERSION BELOW **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-92fb4604426d>:70: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/nathan/miniconda3/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/nathan/miniconda3/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/nathan/miniconda3/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/nathan/miniconda3/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: __init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "('Epoch', 0, 'elbo', -543.90564)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/miniconda3/lib/python2.7/site-packages/matplotlib/cbook/deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch', 1, 'elbo', -171.77632)\n",
      "('Epoch', 2, 'elbo', -165.92232)\n",
      "('Epoch', 3, 'elbo', -160.4344)\n",
      "('Epoch', 4, 'elbo', -156.70665)\n",
      "('Epoch', 5, 'elbo', -154.42703)\n",
      "('Epoch', 6, 'elbo', -152.12761)\n",
      "('Epoch', 7, 'elbo', -151.27179)\n",
      "('Epoch', 8, 'elbo', -149.88364)\n",
      "('Epoch', 9, 'elbo', -148.21837)\n",
      "('Epoch', 10, 'elbo', -147.45747)\n",
      "('Epoch', 11, 'elbo', -147.02655)\n",
      "('Epoch', 12, 'elbo', -146.3809)\n",
      "('Epoch', 13, 'elbo', -146.2698)\n",
      "('Epoch', 14, 'elbo', -146.04372)\n",
      "('Epoch', 15, 'elbo', -146.0089)\n",
      "('Epoch', 16, 'elbo', -144.63922)\n",
      "('Epoch', 17, 'elbo', -144.6196)\n",
      "('Epoch', 18, 'elbo', -144.55756)\n",
      "('Epoch', 19, 'elbo', -144.19975)\n"
     ]
    }
   ],
   "source": [
    "# Full example for my blog post at:\n",
    "# https://danijar.com/building-variational-auto-encoders-in-tensorflow/\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tfd = tf.contrib.distributions\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def make_encoder(data, code_size):\n",
    "  x = tf.layers.flatten(data)\n",
    "  x = tf.layers.dense(x, 200, tf.nn.relu)\n",
    "  x = tf.layers.dense(x, 200, tf.nn.relu)\n",
    "  loc = tf.layers.dense(x, code_size)\n",
    "  scale = tf.layers.dense(x, code_size, tf.nn.softplus)\n",
    "  return tfd.MultivariateNormalDiag(loc, scale)\n",
    "\n",
    "\n",
    "def make_prior(code_size):\n",
    "  loc = tf.zeros(code_size)\n",
    "  scale = tf.ones(code_size)\n",
    "  return tfd.MultivariateNormalDiag(loc, scale)\n",
    "\n",
    "\n",
    "def make_decoder(code, data_shape):\n",
    "  x = code\n",
    "  x = tf.layers.dense(x, 200, tf.nn.relu)\n",
    "  x = tf.layers.dense(x, 200, tf.nn.relu)\n",
    "  logit = tf.layers.dense(x, np.prod(data_shape))\n",
    "  logit = tf.reshape(logit, [-1] + data_shape)\n",
    "  return tfd.Independent(tfd.Bernoulli(logit), 2)\n",
    "\n",
    "\n",
    "def plot_codes(ax, codes, labels):\n",
    "  ax.scatter(codes[:, 0], codes[:, 1], s=2, c=labels, alpha=0.1)\n",
    "  ax.set_aspect('equal')\n",
    "  ax.set_xlim(codes.min() - .1, codes.max() + .1)\n",
    "  ax.set_ylim(codes.min() - .1, codes.max() + .1)\n",
    "  ax.tick_params(\n",
    "      axis='both', which='both', left='off', bottom='off',\n",
    "      labelleft='off', labelbottom='off')\n",
    "\n",
    "\n",
    "def plot_samples(ax, samples):\n",
    "  for index, sample in enumerate(samples):\n",
    "    ax[index].imshow(sample, cmap='gray')\n",
    "    ax[index].axis('off')\n",
    "\n",
    "\n",
    "data = tf.placeholder(tf.float32, [None, 28, 28])\n",
    "\n",
    "make_encoder = tf.make_template('encoder', make_encoder)\n",
    "make_decoder = tf.make_template('decoder', make_decoder)\n",
    "\n",
    "# Define the model.\n",
    "prior = make_prior(code_size=2)\n",
    "posterior = make_encoder(data, code_size=2)\n",
    "code = posterior.sample()\n",
    "\n",
    "# Define the loss.\n",
    "likelihood = make_decoder(code, [28, 28]).log_prob(data)\n",
    "divergence = tfd.kl_divergence(posterior, prior)\n",
    "elbo = tf.reduce_mean(likelihood - divergence)\n",
    "optimize = tf.train.AdamOptimizer(0.001).minimize(-elbo)\n",
    "\n",
    "samples = make_decoder(prior.sample(10), [28, 28]).mean()\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data/')\n",
    "fig, ax = plt.subplots(nrows=20, ncols=11, figsize=(10, 20))\n",
    "with tf.train.MonitoredSession() as sess:\n",
    "  for epoch in range(20):\n",
    "    feed = {data: mnist.test.images.reshape([-1, 28, 28])}\n",
    "    test_elbo, test_codes, test_samples = sess.run([elbo, code, samples], feed)\n",
    "    print('Epoch', epoch, 'elbo', test_elbo)\n",
    "    ax[epoch, 0].set_ylabel('Epoch {}'.format(epoch))\n",
    "    plot_codes(ax[epoch, 0], test_codes, mnist.test.labels)\n",
    "    plot_samples(ax[epoch, 1:], test_samples)\n",
    "    for _ in range(600):\n",
    "      feed = {data: mnist.train.next_batch(100)[0].reshape([-1, 28, 28])}\n",
    "      sess.run(optimize, feed)\n",
    "plt.savefig('vae-mnist.png', dpi=300, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray([elbo, code, samples]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 300, 63)\n"
     ]
    }
   ],
   "source": [
    "print(batch_x[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(595, 300, 30)\n"
     ]
    }
   ],
   "source": [
    "print(train_data[:,:,:60].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
